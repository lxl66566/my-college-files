#import "../../ECUST-typst-template/template.typ": setup-lovelace, algorithm, project, three_line_table, indent
#import "@preview/codelst:2.0.2": sourcecode
#import "@preview/i-figured:0.2.4": show-figure

#let algorithm = algorithm.with(supplement: "算法")

#show: project.with(
  anonymous: false,
  title: "基于 Agentic RAG 的",
  title2: " AI 教学辅助设计系统",
  author: "xxx",
  abstract_zh: [
    随着人工智能在教育领域的深入应用，LLM（大型语言模型）展现出巨大潜力，但在直接用于教学内容生成时，仍面临“幻觉”、知识更新滞后等挑战。针对这些问题，本文设计并实现了一个基于Agentic RAG（智能体检索增强生成）的AI教学辅助设计系统。该系统深度融合LLM的生成能力、RAG的外部知识检索能力以及Agent的规划、工具调用与迭代优化能力。系统实现了智能知识库构建、Agentic RAG驱动的内容生成、多模态内容表达以及对多种在线和本地LLM的灵活集成。研究结果表明，该系统能够有效结合LLM的生成能力与外部知识，通过Agent的智能调度，生成高质量、结构化的教学内容，为教师提供智能、高效、可靠的教案设计。
  ],
  abstract_en: [
    With the deepening application of artificial intelligence in the field of education, large language models (LLM) have demonstrated significant potential. However, when directly applied to teaching content generation, they still face challenges such as "hallucinations" and outdated knowledge. To address these issues, this paper designs and implements an AI-assisted teaching design system based on Agentic Retrieval-Augmented Generation (Agentic RAG). The system deeply integrates the generative capabilities of LLM, the external knowledge retrieval ability of RAG, and the planning, tool invocation, and iterative optimization capabilities of agents. The system enables intelligent knowledge base construction, Agentic RAG-driven content generation, multimodal content presentation, and flexible integration of various online and local LLM. The research results show that the system can effectively combine the generative power of LLM with external knowledge, leveraging agent-based intelligent scheduling to produce high-quality, structured teaching materials, thereby providing teachers with smart, efficient, and reliable lesson plan design.
  ],
  keywords_zh: ("大型语言模型", "检索增强生成", "AI教学辅助", "智能体"),
  keywords_en: ("Large Language Models", "RAG", "AIEd", "Agent"),
  school: "信息科学与工程学院",
  department: "电子与通信工程系",
  major: "信息工程",
  grade: "2021级",
  id: "xxxxxxxx",
  mentor: "xxx",
  class: "xxx",
  date: (2025, 6, 8),
  作者声明日期: (2025, 5, 16),
  作者签名图片: image("../../ECUST-typst-template/assets/signature.png", width: 80pt),
)

#show raw: set text(font: ("Consolas", "SimSun"))

= 绪论

== 研究背景与意义

随着人工智能理论和技术的不断突破，特别是机器学习与自然语言处理领域的进步，AI在教育领域（AIEd）的应用前景广阔。早期AIEd探索的典型代表是智能辅导系统（Intelligent Tutoring Systems, ITS），它们旨在模仿人类教师的辅导过程，为学生提供个性化学习路径和即时反馈 @vanlehn2011relative。这些系统通过对学生的学习行为进行建模，分析其知识掌握情况和潜在困难，从而提供针对性指导。尽管ITS在特定学科和技能训练中取得了显著成效，但其高昂的开发成本、复杂的知识库构建过程以及在处理开放性问题和创意任务方面的局限性，使得它们难以广泛应用于复杂多变的教学场景，尤其是在对灵活性和创造性要求较高的教学设计等领域。

近年来，以深度学习为核心的AI技术取得了革命性进展，进而催生了LLM的问世。基于Transformer架构的GPT系列模型 @vaswani2017attention@brown2020languagemodelsfewshotlearners 通过在海量文本数据上的预训练，展现出卓越的自然语言理解、生成、推理和知识整合能力。这为教育领域带来了全新的机遇，尤其是在文本生成与对话系统方面。已有研究表明，LLM能辅助教师完成重复性的文本工作，如初步资料搜集、草拟教学大纲乃至生成练习题目等 @kasneci2023chatgpt。它们能够理解复杂的教学指令，并根据要求生成结构相对完整的文本内容，这为教学内容的自动化生成提供了可能性。

尽管LLM功能强大，但在直接应用于严肃的教学辅助任务，特别是教学内容生成时，仍面临一系列固有限制。首先，LLM可能产生“幻觉”现象，即生成看似合理但实际不准确或与事实不符的信息 @ji2023survey。在教学内容生成中，知识的准确性和权威性至关重要，任何错误信息都可能误导学生，这是绝不能容忍的。其次，LLM的知识通常仅限于其预训练数据的时间点，对于最新的学科进展或特定教学资源可能无法及时更新，导致生成内容缺乏时效性。最后，标准LLM在缺乏特定上下文或领域知识库引导的情况下，其生成内容的深度和专业性可能无法满足特定教学场景的精细化需求，难以与具体的课程标准和教学资源紧密结合。

为克服LLM上述局限性，检索增强生成（Retrieval Augmented Generation, RAG）技术 @lewis2021retrievalaugmentedgenerationknowledgeintensivenlp 应运而生。RAG将LLM的文本生成能力与外部知识库的检索能力相结合。它通过从可靠知识源检索相关信息片段并将其作为上下文（Context）提供给LLM，引导模型基于这些外部知识进行内容生成。通过这种方式，RAG能够显著提高生成内容的准确性、时效性和相关性，有效减少幻觉现象，并使LLM的输出锚定在特定的知识范围和事实依据之上 @gao2024retrievalaugmentedgenerationlargelanguage。在教学内容生成场景中，RAG可从指定的课程标准、教材内容、优秀教学案例库中检索信息，为LLM提供坚实的内容基础，从而生成更贴合实际教学需求的教案。

本研究旨在设计并实现一个基于Agentic RAG的教学辅助系统，专注于教案的自动化智能化制作。其研究意义主要体现在理论探索与实践应用两个层面：

在理论层面，本研究拓展了LLM在教育领域的应用边界。通过引入RAG机制，本研究探索了如何将LLM的生成能力与外部权威知识库的准确性相结合，以期生成更为可靠、专业的教案内容。这对于提升LLM在知识密集型教育任务中的可信度和实用性具有理论参考价值@lo2025impact。此外，本研究将“Agent”的概念融入RAG框架，代表了AI系统在复杂任务规划与迭代优化能力方面的一次积极尝试。

实践意义则更为显著。首先，本研究能显著减轻教师的备课负担。教案撰写是一项耗时耗力的工作，本系统通过自动化初步的内容组织、活动设计等环节，可以帮助教师节省大量时间，使其能更专注于教学策略的创新、学生差异化需求的满足以及课堂互动质量的提升。
其次，系统通过整合优质教育资源和先进教学理念，有望提升教案的整体质量和规范性。Agentic RAG遵循预设的教学目标和框架生成教案，有助于确保教学内容的准确性和教学设计的科学性，从而间接提升教学效果和学生学习体验。
此外，该系统可以作为教师专业发展的辅助工具，通过提供多样化的教案模板、教学策略建议和即时反馈，激发教师的教学创新思维，促进其教学能力的持续提升。

== 国内外研究发展概况

=== 大型语言模型的教学应用创新

人工智能技术在教育领域的应用已经历了长期发展。早期研究主要聚焦于开发能够部分模拟教师功能的智能系统，这些技术成果广泛应用于多个教学环节。具体而言，自适应学习系统能够基于学生的学习情况动态调整教学内容和进度安排 @ZawackiRichter2019。在辅导环节，智能辅导系统通过模拟一对一教学场景，为学生提供个性化的问题解答和学习指导 @Nkambou2010。另外，人工智能还被应用于自动化评估、学习资源推荐以及学生数据分析等方面，这些技术应用不仅提升了教学效率，也为实现个性化教学提供了可能。

值得注意的是，传统人工智能教育系统在实际应用中仍存在明显不足。一方面，这类系统缺乏对教育理论的深入理解，仅能通过数据分析进行内容调整，难以像人类教师那样做出灵活判断 @数字教育背景下人工智能赋能成人教育发展研究。另一方面，其交互方式往往较为僵化，无法开展自然流畅的深度对话，这在培养学生高阶思维能力方面存在局限。更关键的是，面对需要主观判断的复杂教学任务时，如开放性题目评价和创意写作指导等，传统系统的表现往往不尽如人意。这些局限性促使研究者寻求更先进的技术解决方案。

近年来，以GPT系列为代表的LLM取得重大突破 @brown2020languagemodelsfewshotlearners。这些模型通过海量文本预训练，展现出卓越的语言理解和生成能力。与早期系统相比，它们能够更好地处理自然语言交互和复杂内容生成，为教育技术创新提供了新的可能性。更重要的是，这些进步不仅代表着技术性能的提升，更可能引发教育模式的根本性变革，引发了关于人工智能如何重塑教学实践的广泛探讨 @kasneci2023chatgpt。

=== 教育智能化的发展趋势

LLM在教育领域展现出独特优势，其应用前景主要体现在三个维度：

（1）在个性化学习方面取得显著进展。不同于传统系统基于预设算法的推荐方式，LLM通过自然对话可以准确识别学生的学习难点和兴趣偏好@sharma2025role。这种能力使其能够生成高度定制化的学习材料和指导建议，有效支持探究式学习。已有研究尝试开发基于LLM的智能辅导伙伴，这些系统能够提供即时反馈和个性化指导@kiesler2023exploringpotentiallargelanguage。

（2）在评估反馈方面实现质的飞跃。传统系统主要处理客观题型，而LLM能够对作文等复杂作业进行深入分析。它们不仅能评估知识掌握程度，还可以评判逻辑性、表达清晰度等维度，并提供建设性反馈。

（3）革新了教学资源开发模式。教师可以利用这些模型快速生成多样化教学材料，包括教案设计、课堂演示文稿和分级练习题等 @baidoo2023education。更有研究尝试将其应用于教材编写过程，通过模拟专家评审加速优质教育资源的开发。这种人机协作模式让教师能够更专注于教学创新。

总体而言，LLM正在重塑教育技术格局，为实现智能化、个性化教育提供了全新可能，也为本研究系统的设计开发奠定了技术基础。

=== 现有教学辅助系统分析

教育科技产品通常可分为面向学生和教师两大类。学生端产品包括语言学习工具、作业辅助平台、职业培训课程和早教应用等。教师端产品则涵盖学习管理系统、学生信息系统、课堂管理软件和评估工具等@AI_in_Education_2024。

在众多应用中，AI辅助教案设计因其提升教学效率的潜力而备受关注。随着LLM的发展，教学内容生成系统展现出新的可能性。然而，这些系统在功能设计和智能化水平方面仍处于探索阶段，存在诸多待解难题。

目前市场上已有多个基于LLM的教案生成工具。以中国市场为例，"豆包"和"讯飞星火"等主流模型都集成了相关功能。这些工具通常根据教师输入的基本信息自动生成教案框架。

初步评估显示，这些系统确实能减轻教师负担。例如，"讯飞星火"通过预设提示词生成教学设计（@fig:讯飞星火），为教师提供基础参考。"豆包"则支持上传参考资料和网络搜索（@fig:豆包），使教案内容更具针对性。

#figure(
  image("assets/案例_讯飞星火.png", width: 60%),
  caption: [
    讯飞星火的教案生成用户界面
  ],
) <讯飞星火>
#figure(
  image("assets/案例_豆包.png", width: 50%),
  caption: [
    豆包的教案生成用户界面
  ],
) <豆包>

然而，现有系统仍存在明显局限。多数系统难以处理复杂教学需求，例如"讯飞星火"依赖模板化提示词，导致教案同质化问题；而"豆包"虽然支持资料上传，但五万字的容量限制也影响了其在大量教学资料中的应用效果。因此，开发更智能的教学内容生成系统成为当前研究的重要方向。

== RAG 技术概述

检索增强生成（Retrieval-Augmented Generation，RAG）是由Facebook AI Research团队在2020年提出的创新性技术框架。Lewis等人@lewis2021retrievalaugmentedgenerationknowledgeintensivenlp 首次将预训练序列到序列模型与稠密向量检索技术相结合，为解决知识密集型自然语言处理任务提供了新思路。该技术采用Dense Passage Retriever（DPR）等稠密检索器@karpukhin-etal-2020-dense，现已成为大语言模型应用领域的重要解决方案。

RAG技术的工作机制具有直观性特征。在进行文本生成前，系统会从预构建的大规模知识库中检索相关文档片段。这些检索结果随后作为上下文信息输入生成模型，从而引导模型输出更准确、信息量更大的内容@lewis2021retrievalaugmentedgenerationknowledgeintensivenlp。如@fig:RAG流程示意图 所示，这种架构显著提升了模型处理问答、摘要等知识密集型任务的能力，有效降低了事实性错误的发生率@DFRAG。

#figure(
  image("assets/RAG技术架构总览2.png", width: 100%),
  caption: [
    RAG技术工作流程示意图
  ],
) <RAG流程示意图>

传统RAG系统通常包含两个关键组件：（1）检索模块，负责从知识库中快速定位并返回与用户查询相关的文档片段；（2）生成模块，通常采用预训练LLM，结合原始输入和检索结果生成最终文本。这种设计使模型能够动态引用外部知识，而非仅依赖内部参数化知识。研究表明，采用RAG技术的LLM能够有效缓解幻觉现象、提升时效性并增强数据安全性@gao2024retrievalaugmentedgenerationlargelanguage@基于RAG的供应链智能问答模型。

== RAG 的优化

=== RAG 优化综述

尽管RAG技术展现出诸多优势，其仍暴露出一些固有的局限性。具体而言，传统的检索方法，无论是基于稀疏向量（如TF-IDF）还是稠密向量（如BERT embedding），在处理复杂查询或涉及特定领域术语时，往往难以精确地召回最相关的知识片段。这可能导致检索结果中混杂噪声信息、遗漏关键细节，直接引发LLM生成的内容出现偏差、不完整，乃至产生事实性错误。正如多项研究共同指出，检索器的性能瓶颈是制约RAG系统整体表现的核心因素之一 @gao2024retrievalaugmentedgenerationlargelanguage。

生成环节所面临的挑战同样不容忽视。即使系统成功检索到相关的上下文信息，LLM在整合这些信息并生成连贯且准确的回答时，依然可能遭遇挑战。例如著名的“中间失焦”问题，即LLM倾向于将注意力更多地集中在输入上下文的开头和结尾部分，从而忽略了中间的关键信息 @liu2023lostmiddlelanguagemodels。当检索到的知识片段之间存在矛盾或信息不一致时，LLM也难以有效地进行裁决与融合。传统 RAG通常采用一种相对简化的“检索-阅读-生成”单向流程，这导致其缺乏对检索质量和最终生成结果进行动态评估与调整的机制。

此外，传统 RAG在处理需要多步推理或涉及复杂任务分解的场景时也可能显得力不从心。例如，要回答一个复杂的综合性问题，可能首先需要检索相关的背景知识，随后再基于这些背景知识进行一系列的逻辑推断或子问题查询。然而，传统 RAG所采用的单次检索与生成模式，难以有效应对此类复杂的任务需求。

鉴于上述局限性，研究者们已从不同层面提出了多种改进策略，旨在推动RAG技术朝着更智能、更高效的方向发展。这些改进措施大致可以归纳为针对检索前、检索中、检索后阶段的优化，以及对整个RAG流程的架构性革新。

在检索前阶段，核心目标在于优化用户输入或知识库本身的质量，从而显著提升后续检索的精准度。查询重写（Query Rewriting）和查询扩展（Query Expansion）是此阶段常见的技术手段 @ma2023query。例如，可以利用LLM对用户的原始查询进行改写，使其表达方式更贴合知识库的特点；或者通过扩展查询中的关键词，以增加召回相关文档的可能性；对知识库进行预处理，例如构建知识图谱或优化索引结构等，也能从根本上改善检索质量。

检索阶段，研究的重心则转向提升检索算法的整体性能以及召回相关文档的能力。混合检索（Hybrid Retrieval）技术通过结合稀疏检索与稠密检索的各自优势，能够在不同应用场景下实现更优的平衡。部分研究还深入探索了对检索器进行微调的方法，旨在使其能更好地适应特定领域或任务的需求。还有研究引入重排序（Re-ranking）模块对初步检索到的文档列表进行二次排序，从而将最相关的文档优先置于更靠前的位置，以提升送入LLM的上下文质量 @chen2024attention@rathee2025guidingretrievalusingllmbased。

检索后阶段，核心任务聚焦于如何更有效地利用已检索到的信息。上下文压缩（Context Compression）技术旨在从冗长的检索文档中精准提取关键信息，以减少无关噪声的干扰，并同时适应LLM有限的上下文窗口限制 @li2023compressingcontextenhanceinference。信息融合（Information Fusion）则侧重于处理来自多个检索文档的信息，尤其是在这些信息存在冗余、互补甚至相互矛盾的情况下有效地整合它们以支持最终的生成任务。此外，部分研究已开始探索迭代式检索与生成（Iterative Retrieval and Generation）机制，这意味着系统能够根据初步生成的结果或中间状态，动态地调整后续的检索策略，从而形成一个有效的反馈闭环。

除了上述针对特定阶段的优化策略，对RAG整体架构的革新也已成为一个发展方向。例如，模块化RAG（Modular RAG）和自适应RAG（Adaptive RAG）等概念相继被提出，它们强调根据具体的任务需求和数据特点，灵活地组合并调整RAG的各个组件 @zhao2024retrieval。以CRAG（Corrective Retrieval Augmented Generation）为例，该框架引入了对检索结果的自我修正机制。它通过评估检索文档的相关性，智能地决定是否需要触发新的检索操作，或者利用网络搜索来补充缺失信息，从而显著提高了生成内容的鲁棒性 @yan2024correctiveretrievalaugmentedgeneration。Self-RAG则通过引导LLM进行自我反思，使其能够主动判断何时以及检索何种内容。同时，它还对检索到的文档进行相关性评估与事实性检查，从而全面提升输出的质量与事实准确性 @asai2023selfraglearningretrievegenerate。

=== Agentic RAG 的优势

在众多针对RAG技术的优化路径中，本教学辅助系统决定将其核心架构建立在Agentic RAG之上。Agentic RAG通过将LLM驱动的Agent与传统的RAG流程进行深度整合，显著增强了系统的自主规划、工具调用、动态决策以及自我反思与改进的能力 @singh2025agenticretrievalaugmentedgenerationsurvey。教学内容的生成并非一个简单的过程，它本质上是一个复杂且结构化的认知任务。这远不止于简单的问题解答，更要求系统能够依据教学目标、学科内容等多重因素，进行系统性的规划与富有创造性的设计。

Agentic RAG的独特优势与本系统的具体需求实现了高度契合：

（1）任务分解与规划能力。 教学内容的生成过程能够被有效地细分为多个独立的子任务。这些子任务可能包括：明确教学目标、筛选教学内容、设计教学活动、编撰练习题目以及制定评估方案等。Agentic RAG架构中的Agent凭借其固有的规划能力，能够将诸如“制作一份关于基尔霍夫定律的教案”这类高级指令，拆解为一系列具体且可执行的步骤。随后，针对每个步骤，系统可以灵活地调用相应的RAG流程或外部工具进行处理。

（2）迭代与反思机制。 一份高质量的教案通常并非一蹴而就。它往往需要经过反复的推敲、修订与持续的优化。Agentic RAG机制允许Agent对初步生成的教案内容进行全面的评估与深刻的反思。譬如Agent可以审视教案的逻辑连贯性、教学目标设定的清晰度以及教学活动设计的合理性等多个维度，一旦发现存在不足之处，Agent便能积极地调整检索策略，重新查询相关信息，或直接修正生成的内容。这一过程形成了一个闭环的“思考-行动”迭代循环，从而有效且逐步地提升教案的整体质量。这种动态的自我完善能力，是传统RAG模型所无法比拟，也难以实现的显著优势。

综上，Agentic RAG所具备的任务规划与迭代反思能力，使其成为构建本教学辅助系统较为理想的核心技术方案。这一架构不仅能够有效克服传统传统 RAG模型的固有局限，更能够完美适应教学内容生成所固有的创造性、多步骤及动态调整的复杂任务特性。

== 论文的组织结构

本文围绕“基于 Agentic RAG 的 AI 教学辅助设计系统”展开研究，共分为六个核心章节，系统性地阐述了技术创新点与工程实践。各章节内容架构如下：

第一章：绪论。阐述了以LLM为代表的AI在教学领域方面的应用和发展趋势，分析了现有AI教学辅助系统案例及其局限性，详细介绍了RAG技术的原理、主流改进方向，并论证了采用Agentic RAG作为本系统核心架构的优势。

第二章：系统设计。首先进行了详细的需求分析，明确了系统的核心功能模块与用户期望，随后介绍了系统的总体架构设计，包括前后端分离模式、系统四个层次划分、后端模块结构和数据库表结构设计。

第三章：知识库模块的实现。详细阐述了知识库构建的关键流程与技术选型，重点介绍了针对教学文档特点自主研发的OCR组件，包括文档切分与字符识别技术；讨论了文本分块策略和文本向量化过程中面临的挑战及应对方案；最后，介绍了向量数据库Chroma的选型理由、存储机制、相似性搜索原理以及模型预热等实践经验。

第四章：内容生成模块的实现。深入探讨了系统核心的Agentic RAG内容生成机制，包括介绍了LLM交互接口的设计，详细阐述了轻量级Agent封装库AgenticWrapper的设计与功能，以及为Agent配备的工具函数，再介绍了线性工作流实现，用于编排Agent完成复杂任务。最后讨论了数学公式和SVG图片的生成与前端渲染技术。

第五章：用户界面实现。展示了系统前端的主要界面设计与交互功能。具体介绍了首页的整体布局，内容生成页面的四个模式的交互流程与状态展示，知识库管理页面的文档操作功能，以及模型配置页面如何支持在线API模型和本地模型的选择与管理。

第六章：总结与展望。对全文的研究工作进行了系统性总结，凝练了本系统在Agentic RAG应用于教学辅助领域的主要创新点。同时，分析了系统当前存在的不足，并对未来可能的改进方向和研究前景进行了展望。

= 系统设计

== 需求分析

本教学辅助系统旨在利用 Agentic RAG 技术，为教师提供个性化的教学内容设计与生成支持，减轻备课负担，提升教学设计质量。为实现这一目标，从用户和开发角度出发，系统需要支持以下能力：

- 用户应能方便地上传其教学资料进行索引，包括但不限于TXT、Markdown、PDF（文本型与扫描型）等常见格式。系统需自动对上传文档进行预处理，包括OCR（针对扫描件和图片内容）、文本清洗、语义分块、向量化，并构建高效的向量索引。
- 用户能够对已上传文档进行增、删、改、查操作，并能启用/禁用特定文档参与RAG过程，确保知识库的时效性和相关性。
- 在内容生成阶段，系统需要拥有自主调用知识库检索工具、维基百科查询工具等获取相关信息的能力。
- 系统需要有更多机制以提升最终输出质量，包括但不限于后检查、符号检查、公式与图片输出等。
- 系统应能灵活接入并切换不同的LLM，包括主流的在线商业LLM API（如OpenAI GPT系列、Deepseek系列等）和用户本地部署的开源LLM。

== 总体架构

本教学辅助系统的整体架构采用了当前主流的前后端分离设计模式。该模式通过明确界定用户交互界面与核心业务逻辑处理的职责，提高了开发效率和迭代速度，增强了系统的可维护性。此外，前后端分离架构有利于实现更灵活的系统扩展，例如，后端服务可以独立扩容以应对高并发请求，而前端则可以适配多种客户端设备。同时，通过异步通信和前端渲染优化，能够为用户提供更为流畅和响应迅速的操作体验。

为了实现上述需求并有效组织系统功能，本系统被划分为四个逻辑层次，自顶向下依次为用户界面层、后端服务层、后端逻辑层以及模型赋能层，如@fig:系统总体架构图。

#figure(
  image("assets/架构_总体架构图.png", width: 100%),
  caption: [
    系统总体架构图
  ],
) <系统总体架构图>

用户界面层是系统与最终用户直接交互的门户。它负责向用户呈现系统功能，例如参考资料上传、模型参数配置以及结果的生成、展示与编辑。

后端服务层扮演着前端请求与后端核心逻辑之间的桥梁角色。该层的主要职责是接收来自用户界面层的API请求，对请求参数进行格式校验与类型验证，以确保数据的有效性和安全性。验证通过后，后端服务层会根据请求内容，分发并调用后端逻辑层中相应的处理函数或服务模块，并将逻辑层的处理结果封装后返回给前端。

后端逻辑层是实现本系统核心功能的关键所在，它承载了教学内容生成的完整业务流程。在此层面，系统将执行复杂的任务编排与数据处理，包括但不限于用户需求的解析、知识库的检索与筛选、教学内容的组织与编排、以及基于Agentic RAG 的教案草稿生成。该层通过Agent的智能作业和工作流编排，确保生成的教学内容不仅符合用户需求，同时也满足教学规范和质量标准。

位于架构最底层的是模型赋能层，它为后端逻辑层提供了强大的智能支持。该层集成了多种先进的人工智能模型，协同完成复杂的认知任务。其中包括：（1）LLM模型，用于理解用户意图、生成符合语境的文本内容、进行教学语言的润色与优化；（2）OCR模型，用于从用户上传的图片或扫描文档中提取文本信息，将其转化为可供系统处理的数字格式；（3）文档切分模型，该模型能够识别文档中的不同区域，如标题、段落、公式等，从而帮助系统更精准地定位和提取与教案主题相关的有效信息。这些模型通过标准化的接口被后端逻辑层调用，为其提供了数据理解、信息提取和内容生成等核心能力，是实现本系统智能化教案辅助制作的关键技术支撑。

== 后端模块结构

后端使用 FastAPI 与前端通信。FastAPI 是一个现代化的 Python Restful API 框架，提供高性能的异步 API 服务。除此之外，后端还有用于知识库构建的文档处理模块和用于内容生成的 LLM workflow 模块。

Python模块结构图如@fig:Python模块结构图 所示，后端的 Python 代码大致可分为：API endpoints 与接口类型定义模块用于直接与前端接口交互，并进行数据类型验证；系统服务负责执行接口逻辑；文档处理系统负责提供知识库模块所需的功能；数据库管理模块提供 SQLite 与 Chroma 数据库访问支持；LLM 调用模块负责提供内容生成模块所需的功能。

#figure(
  image("assets/架构_后端文件结构.png", width: 100%),
  caption: [
    Python 模块结构图
  ],
) <Python模块结构图>

系统各模块的具体细节将在后续章节中详细展开。

== 数据库表结构

为了支撑该RAG系统的核心功能，本系统使用了一套精简的数据库结构。该结构主要围绕知识库管理和模型配置两个核心方面展开，共包含三个主要数据表：documents、remote_model_configs 和 current_model_config，以及一个用于维护数据一致性的触发器。

documents 表（@tbl:documents_table）负责存储所有用户上传或系统接入的原始文档信息。每条记录通过唯一的id进行标识，并包含了诸如filename（原始文件名）、type（文档格式，如PDF、DOCX）、size（文件大小）等元数据。为了追踪文档的处理流程，该表还设计了status字段，用于标记文档当前所处的处理阶段（例如：processing, completed, failed）。description字段允许用户为文档添加描述性文本，而enabled字段则提供了临时禁用文档的功能。时间戳created_at和updated_at分别记录了文档的创建时间和最后更新时间，其中updated_at通过一个特定的数据库触发器update_documents_updated_at实现自动更新。此外，message字段用于记录文档处理过程中可能产生的状态信息或错误提示，便于问题排查与用户反馈。

#figure(
  three_line_table((
    ("字段名", "数据类型", "约束", "描述"),
    ("id", "TEXT", "PRIMARY KEY", "文档的唯一标识符"),
    ("filename", "TEXT", "NOT NULL", "原始文件名"),
    ("type", "TEXT", "NOT NULL", "文档类型"),
    ("description", "TEXT", "", "文档的描述信息"),
    ("enabled", "BOOLEAN", "DEFAULT true", "文档是否启用，用于软删除或禁用处理"),
    ("status", "TEXT", "NOT NULL", "文档处理状态"),
    ("progress", "INTEGER", "DEFAULT 0", "文档处理进度（0-100）"),
    ("message", "TEXT", "", "处理状态相关的信息或错误消息"),
    ("size", "INTEGER", "", "文件大小（字节）"),
    ("chunk_size", "INTEGER", "", "分块数量"),
    ("created_at", "TIMESTAMP", "DEFAULT\nCURRENT_TIMESTAMP", "记录创建时间"),
    ("updated_at", "TIMESTAMP", "DEFAULT\nCURRENT_TIMESTAMP", "记录最后更新时间"),
  )),
  kind: table,
  caption: "documents 表结构",
) <documents_table>

// | 字段名 (Field Name) | 数据类型 (Data Type) | 约束 (Constraints) | 描述 (Description) |
// | ------------------ | ------------------- | ------------------------------- | --------------------------------------------- |
// | `id` | `TEXT` | `PRIMARY KEY` | 文档的唯一标识符 |
// | `filename` | `TEXT` | `NOT NULL` | 原始文件名 |
// | `type` | `TEXT` | `NOT NULL` | 文档类型（如PDF、DOCX、PPT等） |
// | `description` | `TEXT` | | 文档的描述信息 |
// | `enabled` | `BOOLEAN` | `DEFAULT true` | 文档是否启用，用于软删除或禁用处理 |
// | `status` | `TEXT` | `NOT NULL` | 文档处理状态（例如：`processing`, `completed`, `failed`） |
// | `progress` | `INTEGER` | `DEFAULT 0` | 文档处理进度（0-100） |
// | `message` | `TEXT` | | 处理状态相关的信息或错误消息 |
// | `size` | `INTEGER` | | 文件大小（字节） |
// | `created_at` | `TIMESTAMP` | `DEFAULT CURRENT_TIMESTAMP` | 记录创建时间 |
// | `updated_at` | `TIMESTAMP` | `DEFAULT CURRENT_TIMESTAMP` | 记录最后更新时间 |


remote_model_configs 表（@tbl:remote_model_configs_table）专门用于存储外部（远程）LLM模型的API密钥信息。它以provider（服务提供商的名称，如OpenAI）作为主键，并存储对应的api_key。这种设计使得系统可以方便地集成和管理多个第三方模型服务。

#figure(
  three_line_table((
    ("字段名", "数据类型", "约束", "描述"),
    ("provider", "TEXT", "NOT NULL, PRIMARY KEY", "服务提供商的唯一标识"),
    ("api_key", "TEXT", "NOT NULL", "对应服务提供商的API密钥"),
  )),
  kind: table,
  caption: "remote_model_configs 表结构",
) <remote_model_configs_table>


与之配合的是current_model_config 表（@tbl:current_model_config），该表设计为仅包含一条记录，用以指明当前Agentic RAG系统正在激活使用的模型配置。该表中的type字段标明了当前模型是remote（远程API模型）还是local（本地模型）。name字段记录了当前模型的具体名称。如果模型类型为remote，则provider字段会引用remote_model_configs表中的服务提供商标识，从而使系统能够获取相应的API密钥来调用远程模型。这种设计允许系统在不同的模型（无论是本地模型还是不同提供商的远程模型）之间进行动态切换，以适应不同的任务需求。

#figure(
  three_line_table((
    ("字段名", "数据类型", "约束", "描述"),
    ("type", "TEXT", "NOT NULL", "模型类型：remote（远程）或 local（本地）"),
    ("name", "TEXT", "NOT NULL", "当前激活使用的模型名称"),
    ("provider", "TEXT", "", "指明其服务提供商\n关联 remote_model\n_configs"),
  )),
  kind: table,
  caption: "current_model_config 表结构",
) <current_model_config>

= 知识库模块的实现

知识库模块负责从用户上传的资料中提取文本内容，并将其转换为向量数据库中存储的向量格式。

知识库模块主要由 OCR（Optical Character Recognition，光学字符识别）组件、文本分块组件、文本向量化组件、向量数据库组件组成，如@fig:知识库构建流程图。

#figure(
  image("assets/知识库构建流程图.png", width: 100%),
  caption: [
    知识库构建流程图。(1)OCR (2)文本分块 (3)文本向量化 (4)存入向量数据库
  ],
) <知识库构建流程图>

用户上传的文档主要分为纯文本与非文本类型。当后端接收到用户上传的文件后，首先会为文件分配一个唯一的 UUID 并保存到 uploads 文件夹内，将文件信息写入 SQLite 数据库；接着，后端会判断该文件的纯文本或非文本类型，对于非文本类型的文件则会对其进行 OCR。最后对纯文本内容或完成 OCR 的非文本内容，按照文本分块、文本向量化、存入向量数据库的步骤完成知识库的构建。在知识库构建过程中，后端会不断更新文档的构建状态，而前端会不断轮询该状态并展示给用户，直至构建完成或因错误而中止。

系统对文件是纯文本还是非文本的判断，主要基于其文件扩展名：

- 对于 .md 和 .txt 后缀，系统会直接认定为纯文本。
- 对于 .doc、.docx、.ppt、.pptx 等文件，系统会读取并提取其内部的文本信息，并将其视为纯文本数据。
- 对于 .pdf 文件的判断逻辑则更为精细：它只有在所有*非空白页*中，*包含可识别文字的页面比例高于 50%* 的情况下，才会被系统识别为纯文本类型，否则属于非文本类型。

== OCR 组件

本系统主要面向教学领域，而教师给出的资料，例如教科书、讲义、习题集等关键教学资源常以PDF扫描件的形式存在，因此需要一个 OCR组件来识别扫描件图片中的文本内容，并将其转换为可编辑的文本格式。

传统的知识库构建流程通常直接采用通用OCR技术处理图像中的文本。然而，教学资料的特殊性在于其内容复杂多样，往往包含大量的数学公式、化学方程式、物理图示、以及各类专业图表。这些非标准文本元素对于传统OCR引擎构成了严峻挑战，识别错误或直接产生无法解读的乱码（如标点符号、特殊字符的误识）屡见不鲜。如@fig:PaddlePaddle直接使用效果 所示，即便是开源领域领先的PaddlePaddle V4模型，在处理图文公式混排的典型教学文档时，其OCR效果亦不尽理想。这种低质量的识别结果，不仅严重干扰了后续文本向量化过程的准确性，也极大地削弱了文本检索的有效性与LLM生成回复的质量。

#figure(
  image("assets/传统OCR效果.png", width: 100%),
  caption: [
    直接将 PaddlePaddle V4 OCR 模型应用于教学资料的效果
  ],
) <PaddlePaddle直接使用效果>

针对上述挑战，一种更为精细化的处理策略是采用基于文档版面分析的分割优先方法。该策略首先对文档页面进行智能切分，将页面解构为文本、公式、图表等不同类型的区域块。随后，针对各类区域块的特点，调用相应的领域特化OCR技术进行识别，例如，针对公式区域采用专门的公式 OCR 模型（例如 SimpleTex），针对文本区域采用通用中文文本OCR。最终，根据下游任务的向量化需求，选取并整合所需部分的识别结果。本系统正是借鉴并实践了这一思路，并在初步实验中观察到了显著的性能提升。

在探索现有技术方案时，笔者注意到开源社区中已涌现出若干尝试采用文档分割策略的OCR工具。然而，经过对代表性工具如Surya和MinerU的细致评估与测试，发现它们在满足本系统特定需求方面仍存在不足。

Surya作为一款设计目标宏大的文档OCR工具包，其功能涵盖文本检测、版式分析乃至LaTeX OCR等多个方面。其核心依赖于定制的神经网络编解码器进行文档分割与识别。尽管其功能全面，但Surya即使在相对基础的纯文本PDF检测任务上，其中文识别准确率都无法达到 100%，更不用说在本系统测试场景中，扫描件通常质量不佳且版面复杂，对OCR模型带来了更大的挑战。

另一款工具MinerU，则旨在将多种文档格式统一转换为机器可读的Markdown或JSON。它采用YOLOv11进行文档切分，并结合PaddleOCR进行文本识别。但在实际部署与集成过程中，暴露出其接口设计缺乏灵活性，仅提供全文档解析，难以针对特定模块进行定制或扩展的问题。此外，其源代码大量使用 Any 类型，在工程规范性方面较为不足，增加了二次开发的难度；更重要的是，其输出的部分公式并非严格的LaTeX格式，导致在后续的Markdown渲染环节出现错误渲染问题。

鉴于现有开源工具在处理复杂教学文档时的局限性，以及为了确保OCR模块能与本教学辅助系统的整体架构紧密集成并满足特定的性能与格式要求，本系统使用自主研发的特化OCR组件。此组件旨在构建一个既能有效应对教学资料多样性，又能灵活适应本系统特定需求的OCR解决方案，为后续的Agentic RAG提供较高质量的文本输入。

=== 文档切分

文档切分是一种文档图像分析（Document Image Analysis, DIA）实践，它将文档按照布局信息切分为不同的块，例如标题块，文本块、公式块、图表块等，然后对每个块进行分别处理。

文档切分主要有两种思路，一种是使用传统模型技术，一种是使用 VLM（Vision-Language Model）技术。

*传统模型技术*：主要是使用 CNN/RNN/GCN/Yolo 等传统深度学习模型，对文档图像进行块识别与分类。典型的例子是LayoutParser@layoutparser，它提供了一套预训练的深度学习模型，例如 Faster R-CNN@fasterrcnn 和 Mask R-CNN@he2017mask，用于快速检测文档图像中的布局。而 RAGFlow 的 deepdoc 模块则是通过训练一个YOLOv10模型实现了页面布局检测。

*VLM 技术*：主要是使用 VLM 模型，对文档图像进行块识别与分类。例如 LayoutLM@layoutlm，它受BERT模型的启发，使用预训练的 MVLM（Masked Vision-Language Model） 模型，对文档图像进行块识别与分类；抑或是在当今 LLM 大行其道的背景下，使用 LVLM（Large Vision-Language Model）模型配合相应提示词，完成layout切分任务，例如MVL-SIB@mvlsib。

虽然使用VLM技术可以取得更好的识别效果，但是本系统仍然使用传统模型技术，原因如下：（1）传统模型技术在layout切分任务上已经取得了不错的效果（2）传统模型技术在推理速度上比 VLM 技术更快（3）VLM对性能要求过高，传统模型更适合在服务器端进行部署。

使用传统模型技术进行文档切分后的效果如@fig:传统模型技术的layout切分效果，可以看出其已经正确识别了文档中的标题、正文、图片与公式等区域。

#figure(
  image("assets/surya_layout.png", width: 90%),
  caption: [
    传统模型技术的 layout 切分效果
  ],
) <传统模型技术的layout切分效果>

=== 字符识别

在文档图像经过切分处理后，可获得对应不同内容区域的图像片段。本系统在字符识别阶段，重点针对正文部分的图像进行处理。其他非文本或结构化元素（如独立公式、完整表格和图片）则在此阶段被排除，其主要考量在于优化后续文本向量化过程的输入纯度，相关具体原因将在文本向量化章节进一步阐述。

尽管文档切分已旨在分离主要的公式区域，但教科书正文文本的OCR依然面临挑战。这主要是因为教材内容常在段落中嵌入行间公式、希腊字母及各类特殊符号，这些元素的准确识别对OCR模型的性能提出了较高要求。因此，此阶段的识别效果在很大程度上依赖于所选用OCR引擎自身的技术能力。

为评估不同OCR方案在处理此类复杂文本时的表现，本研究对从已切分的文档中提取的文本段落图像进行了识别效果的比较分析。

首先，采用 RapidOCR 工具套件中的 ch_server 检测模型与相应的识别模型进行测试。实验结果（@fig:RapidOCR）表明，该方案对中文字符展现出较高的识别准确率，但在处理文本行内嵌的希腊字母时，其识别效果尚有不足。


#figure(
  image("assets/rapidocr.png", width: 100%),
  caption: [
    RapidOCR 对示例段落的识别结果
  ],
) <RapidOCR>

其次，本研究亦测试了 PaddlePaddle OCR（PP-OCRv4）系列模型，具体包括 ch_PP-OCRv4_det_infer 检测模型与 ch_PP-OCRv4_rec_infer 识别模型。如@fig:PaddleOCR对示例段落的识别结果 所示，作为业界广泛应用的OCR技术之一，PP-OCRv4 在处理相同文本段落时，同样在希腊字母的识别方面出现了一些偏差，例如将某些希腊字母误识别为其他字符。

#figure(
  image("assets/paddleocr.png", width: 100%),
  caption: [
    PaddleOCR 对示例段落的识别结果
  ],
) <PaddleOCR对示例段落的识别结果>


鉴于当前开源OCR模型在处理复杂排版及特殊字符混合文本时存在的固有局限性，本系统在设计上接受了OCR阶段可能引入的识别误差。为应对这一问题，在后续的教学内容生成环节，将通过提示引导LLM，使其在生成教案时注意识别并自动纠正这些潜在的OCR错误。这种策略旨在利用LLM的上下文理解和纠错能力，以弥补上游OCR技术的不足，从而确保最终生成内容的准确性。

=== OCR 最终流程

完成了上面的文档切分和字符识别核心算法，就可以构建 OCR 的最终流程了。

为确保数据统一性与互操作性，所有输入图像均标准化为 PIL Image 格式。本系统采用 surya 的布局分析模块进行文档切分，字符识别任务则由 RapidOCR 完成。

由于 OCR 任务通常耗时较长，系统初期尝试采用多线程并行处理以提升效率。然而 surya 的布局分析模块在多线程并行处理时会有问题，具体表现为 The expanded size of the tensor (13) must match the existing size (12) at non-singleton dimension 3 张量尺寸不匹配的异常。为了解决该问题，本系统采纳了一种混合并行策略：文档的版面切分阶段被设计为串行执行，以确保 surya 模块的稳定性；一旦单页文档的版面切分完成，其所产生的多个图像块即被提交至预先构建的线程池，进行并行字符识别。这样成功规避了surya库的并发限制，保持了高效率的并行识别，显著减少了整体 OCR 处理时间。

== 文本分块

文本分块（chunk）是将整份文档切为一个个小片段，并将其送入向量化模型进行向量化的过程。文本分块的精细度直接影响到索引性能（相关性），过短的分块不利于模型的上下文提取，而过长的分块又会导致文本向量化的效果受到不利影响。

虽然一般的 RAG 系统常采用的文本分块方式都是固定大小分块或基于句子分块，但本系统使用的分块方式是按照段落分块，原因如下：

（1）按照段落进行分块更能保持文本的上下文关联性。段落本身就是表达相对完整语义的单元，其内部句子围绕特定主题或教学环节展开，具有较强的逻辑联系。将逻辑关联紧密的段落拆分到不同的文本块，可能会导致上下文割裂，影响模型对整体语义的把握，进而影响生成教案的完整性和流畅性。

（2）使用段落分块所带来的额外 token 消耗的边际效应是严重的，通常不会形成系统性能的主要负担。与固定大小分块等更细粒度的分块方式相比，按照段落分块可能会导致每个文本块包含更多的内容，最终增加系统的 token 消耗。然而，对于教学内容生成这类需要深度理解和信息整合的任务而言，其本身就需要处理和生成相对较长的文本内容，因此会消耗大量的 token，段落分块所带来的额外 token 消耗是可接受的。

当然，由于文本向量化模型对输入的文本长度具有限制，对于过长的段落，本系统也会使用基于重叠（overlap）的分块方法（@fig:基于重叠的分块方法），对其进行更加细粒度的切分，以确保向量化效果。

#figure(
  image("assets/overlap_chunk.jpg", width: 90%),
  caption: [
    基于重叠的分块方法图示
  ],
) <基于重叠的分块方法>

== 文本向量化

文本向量化的目标是将非结构化的教学文本（如课程标准、教材内容、知识点解析及相关学术文献）转化为机器可计算的稠密向量表征。这些向量能够捕捉文本深层的语义信息，而非仅仅停留在词袋模型或TF-IDF（一种关键词挖掘技术）等方法所关注的表面统计特征。通过文本向量化可以构建一个语义一致的向量空间，在此空间中，文本间的语义相似度主要通过计算其对应向量间的余弦相似度（Cosine Similarity）来度量。余弦相似度关注向量间的方向一致性而非绝对大小，使其能够有效地捕捉并保持文本的核心语义特征，而不易受文本长度或具体词汇选择的影响。因此，语义内涵相近的内容会呈现较高的余弦相似度。这种特性确保了系统在检索时能深入理解查询意图与知识库内容的语义关联，确保召回信息的相关性。

然而，数学公式与自然语言文本在结构、符号体系及语义表达上存在的显著差异，为实现文本公式混排的向量化带来了挑战。主流的词嵌入和文档向量化模型（如Word2Vec, BERT等）主要针对自然语言文本进行训练和优化，直接将其应用于数学公式的效果较差。即使将其应用于文本与公式混排的场景，模型也会将公式内部的符号视为噪声，表征为公式减小了总体语义相关性。在本系统使用的 gte:large-zh-f16 模型上，对两个相似句子的其中之一添加不同长度的公式，比较向量化后的文本距离，结果如@fig:附加公式长度对向量化效果的影响 所示：

#figure(
  image("assets/附加公式长度对向量化的影响.png", width: 100%),
  caption: [
    附加公式长度对向量化效果的影响
  ],
) <附加公式长度对向量化效果的影响>

结果表明，附加公式的长度越长，向量化后的距离越大，相似度越低，表明公式长度越长，对语义相关性的减弱程度越大。这也可能导致索引时的文本相关度下降，最终影响到内容生成的质量。

虽然也有针对数学表达式进行向量化特化的模型@gangwar2022semantic，但该模型并未针对中文语境下的数学教学内容进行特化训练，因此无法直接无缝应用于本系统所面向的中文教学资源。若要针对中文环境重新训练此模型，则需要大量的标注数据和计算资源，其成本过高，因而本系统并未采用此训练方法。

除了向量化模型本身的适应性问题外，公式的准确OCR也是一个难题。数学公式对字符识别的准确率极为敏感，任何一个微小的差错，如字母、数字或符号的错误识别，都可能导致整个公式的语义失真甚至完全错误。目前，虽然存在一些高准确率的公式OCR引擎，例如 SimpleTex，但其通常为闭源服务，难以在本地环境中进行灵活部署和集成。即便技术上允许本地部署，这类高精度模型往往也需要大量的计算资源，这无疑会增加系统的整体复杂性和运行成本，给下游的内容生成带来挑战。

鉴于上述种种原因，本系统将丢弃公式区域，只使用正文内容进行文本向量化与内容生成索引。

== 向量数据库

通过将向量化后的文本映射到多维空间，向量数据库可以对向量的语义相似性进行细致的分析，从而显著提高 RAG 索引参考文本的准确性。相比于遍历所有文本寻找相似性，向量数据库的相似性搜索算法也可以降低时间复杂度，提高搜索效率。

在 RAG 系统中，向量数据库主要起到存储向量化后的文本和提供相似性搜索功能的作用。

=== 向量数据库选型

生产环境中常用的向量数据库很多，例如特化搜索算法的 FAISS，在海量数据上表现优异的Milvus，以及 AI 场景下优化的Weaviate。而本项目使用Chroma向量数据库进行知识库存储。

Chroma 是一个使用 Rust 语言编写的嵌入式向量数据库，其显著优势在于轻量级、易于集成，并提供了便捷的 Python API 接口。Chroma 被设计为“开发者优先”的数据库，强调开箱即用的体验，尤其适合于快速原型开发和中小型规模的应用部署。其核心功能围绕着存储文本嵌入及其元数据，并提供高效的相似性搜索服务。尽管是嵌入式数据库，Chroma 依然注重性能表现，尤其是在其默认配置下，能够为常见的教学资源规模提供令人满意的查询速度。

为满足系统功能需求，向量数据库需要支持对选定文档片段建立索引。为此，向量数据库中的每条文本数据均需标注元数据标签以标识其文档来源。同时，系统要求向量数据库提供的相似性搜索接口必须具备元数据过滤功能，即能够限定在指定文档集合内进行最近邻向量检索。目前的主流向量数据库，包括Chroma均已提供完善的元数据过滤支持。

=== 存储部分

向量数据库可以高效地存储向量化后的文本。Chroma 使用块文件系统来高效地组织数据。该系统基于 Apache Arrow 构建，以实现高效的数据表示，并利用元数据层来管理块引用。

// #figure(
//   image("assets/chroma1.png", width: 100%),
//   caption: [
//     Chroma 的存储结构模块关系图
//   ],
// )

块文件系统采用写时复制 (copy-on-write) 方法，即将更改写入新块，而不是修改现有块。这提高了并发性，并提供了一定程度的版本控制。区块是块文件系统中存储的基本单位，每个区块都是一个独立的单元，具有唯一的 UUID；区块存储系统分为两层，第一层是用于频繁访问的块的内存缓存，第二层是保障数据安全的持久存储。BlockManager（负责创建、分叉和管理数据块的关键组件）可以从存储中获取块并将其缓存在内存中，以加速后续访问。Chroma 还提供了一个 Prefetch 函数，用于通过在需要相关块之前将其加载到缓存中来提高性能。

通过这些机制，Chroma 能够高效管理向量存储，在嵌入式向量数据库领域展现出优秀的性能。

=== 相似性搜索

相似性搜索是指在向量数据库中高效地检索与给定查询的向量嵌入在语义上相似的向量的过程。由于文本向量化的特点，相似性搜索本质上是一个高维空间中的最近邻搜索问题。

在高维向量空间中执行精确的最近邻搜索（Exact Nearest Neighbor Search, ENN）面临着相当大的计算挑战，即所谓的“维度灾难”（Curse of Dimensionality）。当向量维度增加时，数据点会变得异常稀疏，传统的索引结构（如 k-d 树或 R 树）在处理高维数据时性能会急剧下降，其复杂度往往趋向于线性扫描，这对于资料数量巨大的的教学辅助系统而言是难以接受的。如果每次查询都需要对所有文档向量进行暴力比较，其延迟将严重影响用户体验和系统的实用性。

为了克服这一挑战，近似最近邻搜索（Approximate Nearest Neighbor Search, ANN）应运而生，并成为当前主流向量数据库的核心技术。ANN 算法的目标是在可接受的时间内找到与查询向量足够接近的向量，它通过牺牲一定的召回率（Recall）来换取搜索效率的大幅提升。在教学辅助系统的应用场景中，这种牺牲通常是可以接受的，因为找到绝大多数高度相关的教学资源往往比确保找到每一个可能相关的资源更为重要。

在众多的 ANN 算法中，基于图索引的算法近年来表现尤为突出，其中具代表性的便是 HNSW (Hierarchical Navigable Small World，分层导航小世界) 算法。HNSW 构建了一个多层级的图结构，如@fig:HNSW分层。在最顶层，图的连接非常稀疏，节点间的“跳跃”距离很远；随着层级向下，图的连接逐渐变得密集。搜索过程从顶层图的一个或多个预设入口点开始，通过贪心算法在当前层找到局部最近的节点，然后进入下一层，从上一层找到的节点附近开始继续进行更精细的搜索，直至到达最底层。这种分层导航的策略使得 HNSW 能够快速定位到包含查询向量最近邻的区域，并通过在底层进行更细致的搜索来提高准确率。HNSW 算法在构建索引时，通过控制每个节点的最大连接数和邻居选择启发式（通常是选择距离最近的 M 个节点），以及在搜索时控制动态候选列表的大小等参数，可以在搜索速度和召回率之间进行灵活的权衡。

#figure(
  image("assets/hnsw-structural.png", width: 50%),
  caption: [
    HNSW 分层搜索原理示意图。蓝色轨迹代表 HNSW 搜索路径
  ],
) <HNSW分层>

Chroma 等现代向量数据库都内置或支持 HNSW 算法，并对用户屏蔽其复杂性，仅提供简洁的 API 接口进行向量的存储和查询。

除了算法本身，相似性度量方式的选择也至关重要。常见的度量包括余弦相似度（Cosine Similarity）、欧氏距离（Euclidean Distance）和内积（Dot Product）。对于由现代语言模型生成的文本嵌入而言，余弦相似度因其对向量长度不敏感，更能有效捕捉方向上的一致性（即语义相似性），因此被广泛采用。当两个文本向量的方向越接近，它们的余弦相似度就越接近1，表明语义越相近。正是利用了文本嵌入和高效的、基于余弦相似度的 ANN 搜索，才能够根据教师输入的模糊查询，快速从庞大的教学资料库中筛选出最相关的若干片段，为 RAG 框架的后续处理提供高质量的索引内容。

=== 预防超时问题

Chroma 中提供了 ollama_embedding_function 对 ollama 上部署的向量化模型进行集成。然而，首次调用向量化模型时需要将模型加载到内存（若使用 GPU 运行 ollama 则为显存），该过程需要消耗一定时间，根据模型存储介质的不同（SSD 或 HDD），可能为数秒到数分钟不等。而 Chroma 中的 collection.add 函数内置了一个固定的、不可由用户直接配置的超时时间，函数调用超时则会直接返回错误。问题的核心在于，若 collection.add 方法是系统生命周期内的首次调用，那么模型加载所引发的显著延迟极有可能超出该方法内置的超时阈值。一旦发生超时，Chroma 将中断操作并抛出错误，导致数据入库失败。

为确保数据能够稳定可靠地录入 Chroma 数据库，本系统实施了一种“预热”策略。该策略的核心思想是在首次执行collection.add之前，主动向 Ollama 服务发送一个轻量级的向量化请求，该请求通常是一个简短、无实际意义的文本字符串。其关键作用在于强制 Ollama 提前完成目标嵌入模型的加载流程，使其常驻于内存或显存之中并处于就绪状态，无需再经历耗时的磁盘加载过程，其执行时间将低于 Chroma 内部的超时限制，从而消除因模型初次加载延迟导致的超时错误。

模型预热后，在后续的向量化与入库过程中仍然有几率观测到超时问题。该问题主要发生在较大的知识库数据上，经过排查后得知：文本向量化模型在开发环境运行 CPU 上，一次性传入过多文本进行嵌入处理超出了系统的承受能力，导致超时。因此本系统将每次传入嵌入模型的文本数量限制为 100 条，分批次进行嵌入。该上限也可以通过 config 文件修改，以适配不同性能的处理器。

= 内容生成模块实现

内容生成流程作为本教学辅助系统的核心逻辑模块，旨在将用户输入的问题或教学需求，通过结构化处理与智能生成转化为高质量的教学资源。

内容生成模块的子模块架构如@fig:内容生成模块架构 所示。Agent是模块的中间层，也是 Agentic RAG 内容生成的核心。每个Agent内部包含一个LLM统一调用接口，其通过工具调用的能力，能够集成外部知识或执行特定操作。为了保证输出的规范性和可用性，Agent还具备结构化输出的模块。此外，记忆组件的引入使得Agent能够维护上下文信息，提升对话或任务处理的连贯性。而最外层的Workflow由多个 Agent 共同组成，能够编排整个Agentic RAG流程的数据流向；用户展示则用于在后端维护当前状态，并将Workflow各状态呈现给用户。这种架构旨在通过Agent的生成能力，提升生成内容的质量、相关性和可信度。

#figure(
  image("assets/内容生成模块架构.png", width: 60%),
  caption: [
    内容生成模块架构
  ],
) <内容生成模块架构>

接下来将由内到外介绍各个子模块的详细设计与功能。

== LLM 统一接口

为满足教学辅助系统对LLM能力的多样化需求，并兼顾开发效率与部署灵活性，LLM统一接口的设计通常需要具备以下特性：

*模型异构性支持*：系统应能无缝集成不同来源的LLM，包括商业在线API（如OpenAI GPT系列、Anthropic Claude系列等）和用户本地部署的开源模型（如Llama等）。这为用户提供了根据成本、性能、隐私需求选择最合适模型的自由度。

*统一调用范式*：尽管底层LLM的API接口各异，但上层应用逻辑应通过一个统一、抽象的接口进行调用，以降低耦合度，简化开发和模型切换的复杂度。

*异步处理能力*：Agentic RAG系统通常涉及多轮对话、知识检索、内容生成等多个可能耗时的操作。采用异步处理机制能够避免I/O阻塞，提高系统的并发处理能力和用户交互的响应速度。

然而，实现上述目标面临若干挑战，主要包括：如何屏蔽不同LLM服务提供商API的差异性；如何整合对本地部署模型的支持，特别是处理其与主流在线API在调用方式（如同步与异步）上的不一致性。

=== 在线 LLM API 的统一调用

对于在线LLM模型API，本系统选用 litellm 库作为核心的统一调用层。litellm 是一个优秀的开源库，它提供了一个标准化的接口，能够将请求路由到超过100种不同的LLM API，包括OpenAI、Azure OpenAI、Cohere、Anthropic等主流服务。litellm 极大地简化了接入新模型或在不同模型间切换的过程，并将不同API的特有参数（如 model, messages, temperature, max_tokens 等）映射为一套通用参数，降低了学习成本；并且还内置了异步支持，提供了 litellm.acompletion 异步接口，天然符合本系统对异步处理的需求，能够高效地执行非阻塞的API调用。

通过 litellm，系统仅需维护一套调用逻辑，即可与众多在线LLM服务进行交互，显著提升了系统的灵活性和可扩展性。

=== 本地 LLM 模型的集成管理适配

为了支持用户在本地环境中运行LLM，特别是考虑到数据隐私、成本控制或离线使用的场景，本系统集成了对 Ollama 的支持。Ollama 是一个流行的工具，它简化了在本地下载、运行和管理开源LLM（如Llama 2, Mistral, CodeLlama等）的过程。Ollama 将这些模型封装为本地HTTP服务，可通过API进行访问。

本系统采用 python-ollama 库作为客户端，向 Ollama server 发起HTTP请求，从而调用本地部署的LLM。这种方式使得本地模型的调用与在线API的调用在概念上趋于一致（均为网络请求），但仍存在一个关键差异：python-ollama 库默认提供的 ollama.chat 接口是同步的。

在基于 asyncio 的异步系统中，直接调用同步阻塞的 ollama.chat 接口会导致事件循环被长时间占用，从而严重影响系统的并发性能和响应能力。为解决此问题，并使本地模型的调用方式与 litellm.acompletion 的异步特性保持一致，本系统对 python-ollama 库的接口进行了异步封装，如@fig:统一接口异步生成函数。

该封装的核心是利用 asyncio.to_thread 函数。asyncio.to_thread 能够将一个同步阻塞的函数调用提交到独立的线程池中执行，并返回一个可 await 的协程对象。这样，主异步事件循环就不会被阻塞，从而保持了系统的高响应性。封装后的异步生成函数 async_generate 如下：

#figure(
  sourcecode[
    ```python
    async def async_generate(
        self,
        model: str,
        messages: list[dict[str, str]],
        temperature: float = 0.2,
        max_tokens: int | None = None,
    ) -> str:
        """异步生成回复

        Args:
            model: 模型名称
            messages: 消息列表，格式为 [{"role": "user", "content": "hello"}...]
            temperature: 温度参数，控制随机性
            max_tokens: 最大生成token数，None表示不限制

        Returns:
            str: 生成的回复内容
        """
        try:
            # 将同步的 chat 调用包装在一个异步操作中
            response = await asyncio.to_thread(
                self.client.chat,
                model=model,
                messages=messages,
                options={
                    "temperature": temperature,
                    "num_predict": max_tokens if max_tokens else -1,
                },
            )
            return response["message"]["content"]
        except Exception as e:
            raise Exception(f"Ollama generation failed: {str(e)}")
    ```],
  kind: image,
  caption: "统一接口异步生成函数 async_generate 的实现",
) <统一接口异步生成函数>

通过这种方式，无论是调用 litellm.acompletion 还是经过封装的本地 Ollama 接口，上层应用均可使用统一的 await 语法进行异步调用，实现了接口层面的一致性。

=== LLM 抽象层的架构设计

为了进一步封装不同LLM实现的细节，并为系统其他模块提供一个简洁、稳定的服务接口，本系统设计了一个 LLM 类。该类采用单例模式实现全局唯一的访问点，提供了一个统一的核心异步方法 async def async_generate(self, messages: list[dict[str, str]], temperature: float = 0.2, max_tokens: int | None = None)。此方法内部根据 self.current_model 判断是调用 litellm 还是本地 Ollama 客户端的相应异步生成函数。该类还负责加载和管理LLM相关的配置信息，包括当前选定的在线或本地模型，在线模型的API密钥、API端点等。这些配置信息能够从系统数据库中动态加载和更新，确保了系统的灵活性。

== Agent 的实现

=== AgenticWrapper 的功能结构

Agent的角色超越了传统LLM本身，它在LLM的文本生成核心能力之上，集成了工具调用、记忆管理以及结构化输出等关键功能。其中，工具调用机制尤为核心，它允许Agent与外部环境或特定知识库进行交互，从而扩展其应用范围和解决复杂问题的能力。

尽管当前已有多种开源框架支持LLM Agent的构建，但在本项目的具体实践中，直接采用这些框架遇到了一些障碍。首先，本系统需整合通过 ollama 和 litellm 访问的LLM服务，其核心交互依赖于已实现的统一异步接口 async_generate。若要将此特定函数适配于现有Agent框架，往往需要深入理解并重写框架底层抽象，例如 LangChain 中的 BaseLLM 等核心组件，这带来了较高的集成成本。其次，部分现有框架功能集较为庞大，对于本项目的特定需求而言，可能引入不必要的复杂性，并在一定程度上限制了定制化扩展的灵活性。

鉴于以上考量，本系统选择自行研发一个轻量级的Agent封装库：AgenticWrapper。该库的核心设计思想是围绕一个用户自定义的异步LLM调用函数（例如本项目中的 async_generate 函数）进行封装，从而赋予其Agent的核心功能。AgenticWrapper 致力于提供清晰的接口设计、良好的可扩展性，并实现了全面的类型注解。该库在运行时不依赖任何第三方包。

AgenticWrapper 的核心功能体现在以下几个方面：

首先是结构化输出能力。用户可以使用Python的dataclass来定义期望的输出数据结构，Agent可以将LLM生成的文本信息自动解析并填充到预定义的结构中。例如，若定义一个SearchResult数据类，Agent在执行搜索相关任务后，能够直接返回一个SearchResult的实例对象，便于后续程序的处理与验证。示例如@fig:AgenticWrapper结构化输出：

#figure(
  sourcecode[
    ```python
    @dataclass
    class SearchResult:
        query: str
        results: List[str]
        total_count: int

    response = await agent.query("搜索相关内容", structured_output_type=SearchResult)
    assert isinstance(response, SearchResult)
    ```
  ],
  kind: image,
  caption: "AgenticWrapper 结构化输出示例",
) <AgenticWrapper结构化输出>

其次是工具调用机制。开发者可以定义一系列异步Python函数作为Agent可调用的工具，并为这些函数提供清晰的文档注释，以辅助LLM理解工具的功能和使用方式。举个例子（@fig:AgenticWrapper工具调用示例），该示例定义了一个get_weather函数用于查询特定地点的天气信息。在初始化Agent时，将此类工具函数列表传入，Agent便能在后续的交互中，根据用户指令或任务需求，自主决定调用合适的工具以获取信息或执行操作。

#figure(
  sourcecode[
    ```python
    async def get_weather(location: str) -> str:
        """获取天气信息"""
        # 模拟 API 调用
        await asyncio.sleep(0.1)
        if location.lower() == "london":
            return "天气晴朗，气温为 15°C。"
        else:
            return f"我不知道 {location} 的天气情况。"
    agent = Agent(llm_interaction_func=mock_llm_func, tools=[get_weather])
    response = await agent.query("查询 london 的天气")
    print(response)
    ```
  ],
  kind: image,
  caption: "AgenticWrapper 工具调用示例",
) <AgenticWrapper工具调用示例>

此外，AgenticWrapper 也内置了记忆管理功能，允许Agent在多轮对话中保持上下文连贯性。例如，它提供了如clear_memory()这样的方法，用于显式清除或重置对话记忆。

在工具函数的实现上，当前版本的AgenticWrapper要求工具函数的输入参数为任意 json 标准支持的类型（list, dict, int, float, str, bool, None 及其嵌套结构），返回值为字符串（str）类型。Agent 通过 Python inspect 模块获取工具函数的输入参数和返回值的类型信息，并将其作为提示词的一部分，告知LLM。如果LLM需要调用工具，则会返回一个包含工具名称和输入参数的 json 对象。Agent 收到此对象后，会通过反序列化获取工具与输入参数，并进行工具函数调用。

通过AgenticWrapper的实现，本项目能够在一个统一且可控的框架内，高效地利用底层LLM接口，并赋予其必要的Agent能力。

该模块代码已在 Github 开源，地址为 #link("https://github.com/lxl66566/AgenticWrapper")。

=== 工具函数设计

本系统为 AgenticWrapper 配备了一系列工具函数。这些工具函数赋予Agent与外部知识源交互的能力，是实现Agentic RAG的关键环节。Agent通过调用这些预定义的工具，能够自主地获取、筛选和整合信息，以构建结构合理、内容丰富的教学内容。

（1）搜索知识库中的相关信息。系统将向量空间中的最近邻搜索作为工具函数提供给 Agent，其签名为 vector_db_search(content: str, max_num: int = 5) -> str。content代表Agent根据当前教案生成任务的需求，动态构建的查询字符串。这可能是一个具体的知识点或是一段描述性文本，用以引导检索方向。max_num指定返回最相关信息片段的数量，使LLM可以根据对信息量的需求自行调整信息获取数量。

（2）爬取维基百科信息。其签名为 wikipedia_search(nounce: str) -> str。此工具用于从维基百科这一开放的在线百科全书中获取特定名词、概念、事件或人物的信息。维基百科因其覆盖范围广泛、信息更新相对及时、条目结构化程度较高以及多语言支持等特点，成为获取普适性背景知识和通用定义的理想外部知识源。此工具使用 Wikipedia-API 包进行维基百科的页面内容获取与解析。

== 工作流的实现

本系统中的工作流（Workflow）层级构建于Agent之上。其核心职责在于精确编排数据在这些 Agent 实例间的线性流转路径，并对它们进行有序的组织与管理，为内容生成构建了清晰的顺序执行框架。

#figure(
  image("assets/workflow.png", width: 80%),
  caption: [
    内容生成流程图
  ],
) <内容生成流程图>

教学内容的生成遵循一个明确的线性流程，如@fig:内容生成流程图 所示。首先，用户的输入作为初始信息被传递给工作流中的第一个 Agent。该 Agent 处理用户输入并补充相关信息。随后，此结果作为后续内容生成 Agent 的输入，内容生成 Agent 在其执行阶段可能会利用其内置能力（如从向量数据库或外部知识源检索信息）来处理当前输入并生成内容。在内容生成 Agent 成功执行后，其Markdown 格式输出结果会经过一个处理步骤，例如移除思考过程的标记（\<think\>\</think\>）和 SVG 生成与预处理（后文将详细介绍），以确保格式的规范性。这个串行过程持续进行，直到工作流中所有的 Agent 都执行完毕，或者某个 Agent 在执行过程中遇到不可恢复的错误。最终，汇集所有成功执行的 Agent 的输出，形成完整的结果呈现给用户。

为了更精细地控制 Agent 的线性执行流程、状态追踪以及与用户界面的交互，本系统设计并实现了一个 Workflow 类。该类专门用于管理一系列 Agent 实例，确保它们按照预定义的顺序依次执行。

Workflow 类的核心架构基于一个简单的线性序列来组织各个 Agent 节点。选择线性结构是为了简化任务编排逻辑，使得数据流向清晰直接：前一个 Agent 的输出严格作为后一个 Agent 的输入。系统中 Agent 的核心处理函数设计为异步操作，这使得单个 Agent 在执行其内部任务时不会阻塞整个应用程序，但 Agent 之间的执行是严格串行的。

在用户交互层面，工作流的状态通过一个外部列表（一个 WorkflowType 对象的 steps 属性，其中每个元素是 WorkflowStep 实例）进行同步和展示。这个列表由 Workflow 类在内部进行管理和更新，直接反映了各个 Agent 的执行步骤和状态。Workflow 类在初始化时，会根据传入的 Agent 列表构建这个步骤列表的初始状态，每个 Agent 对应一个 WorkflowStep，初始状态通常为“等待中”。

当工作流运行时，对应 Agent 的 WorkflowStep 状态会依次更新为“处理中”、“已完成”或“错误”。特别地，由于单个 Agent 在其执行过程中可能调用外部工具，为了向用户清晰展示这些内部操作，Workflow 类具备动态调整步骤列表的能力。若一个 Agent 执行了工具调用，系统会在该 Agent 对应的步骤之后，向步骤列表中插入一个新的 WorkflowStep 来代表该工具调用及其结果。这使得步骤列表的长度可以动态变化，为用户提供更细致的任务进展反馈。为确保前端展示的有序性，该步骤列表会根据步骤的状态及其原始顺序进行排序。

系统的错误处理机制设计为线性传播型。当工作流中的任何一个 Agent 在执行阶段发生错误时，该 Agent 对应的 WorkflowStep 的状态会被标记为“错误”，并记录相关的错误信息。一旦发生错误，整个工作流将立即停止后续 Agent 的执行。所有尚未开始执行的后续 Agent 步骤也会被标记为“错误”，并注明是由于上游 Agent 执行失败所致。这种机制有助于快速定位问题并向用户或调用方报告明确的故障点。

Workflow 类的核心用户接口主要通过其构造函数和运行方法体现，如@fig:Workflow类核心用户接口定义_线性：

#figure(
  sourcecode[
    ```py
    class Workflow:
        """
        Workflow 类，用于编排 RAGAgent 实例的线性执行流程，并提供基于 WorkflowStep 列表的状态视图。
        数据流从首个 RAGAgent 流向后续的 RAGAgent。
        """

        def __init__(self, workflow_self: WorkflowType, agents: List[RAGAgent]):
            """
            初始化 Workflow 实例。

            Args:
                workflow_self: 一个外部 WorkflowType 对象，其 'steps' 列表用于同步 workflow 的步骤状态。调用方负责创建和传入此对象。Workflow 将管理此列表的内容和排序。
                agents: 一个 RAGAgent 实例的列表，定义了工作流的线性执行顺序。
            """
            ...

        async def run(self, initial_messages: str | list[str]) -> Dict[str, str]:
            """
            异步执行线性 workflow。

            Args:
                initial_messages: 提供给第一个 RAGAgent 的初始输入消息。可以是一个单独的字符串或一个字符串列表。

            Returns:
                一个字典，包含 workflow 中所有已成功执行 RAGAgent 的最终输出。
                字典的键为 RAGAgent 名称，值为该 Agent 的最终输出字符串。
                未执行或失败的 RAGAgent 不会包含在结果中。
            """
            ...
    ```],
  kind: image,
  caption: [Workflow 类核心用户接口定义],
) <Workflow类核心用户接口定义_线性>

在实际应用中，首先需要准备一个 WorkflowType 的实例（通常其内部 steps 列表为空）和一个包含按期望执行顺序排列的 Agent 实例的列表。然后，使用这两者来实例化一个 Workflow 对象。最后，调用该对象的 run 方法并传入初始输入数据，即可启动整个教学内容生成工作流的异步线性执行。系统会通过之前传入的 WorkflowType 实例的 steps 列表，动态更新每个步骤的状态和结果，供用户界面或其他模块观察。

== 公式生成与渲染

目前，主流的LLM在处理结构化文本，尤其是包含数学表达式的内容时，普遍采用 Markdown 作为其主要的输出格式。Markdown 凭借其简洁的语法和良好的可读性，在开发者社区和内容创作领域得到了广泛应用。而对于在 Markdown 中嵌入数学公式的任务上，LaTeX 公式格式以其兼容性和泛用性已经成为了事实上的标准。本系统在设计之初，便充分考虑了这一技术趋势，选择了 Markdown 作为内容交互的基础格式，并采用 LaTeX 语法描述数学公式，以期最大限度地利用现有成熟的技术生态。

为了在用户界面展示LLM生成的包含 LaTeX 公式的 Markdown 内容，本系统在前端渲染层面引入了成熟的 react-markdown 作为核心的 Markdown 解析与渲染引擎。react-markdown 提供了灵活的插件机制，允许开发者根据需求扩展其功能。为实现 LaTeX 公式的正确渲染，本系统进一步集成了 remark-math 和 rehype-katex 这两个关键插件。remark-math 插件负责在 Markdown 解析阶段识别并处理 LaTeX 语法标记的数学公式，rehype-katex 插件则在 HTML 转换阶段介入，利用 KaTeX 数学排版库，将这些特定节点高效地渲染为可在浏览器中清晰展示的数学公式。通过这一系列组件，系统得以将大模型输出的 Markdown 文本与其中内嵌的复杂 LaTeX 公式高质量地呈现给最终用户。

除了通用的公式渲染能力，本系统还特别针对中国教育体系的实际需求，设计并实现了一个关键的后处理（postprocessing）步骤。LLM在生成数学公式时，其训练数据可能更多地来源于国际学术文献或西方教材，因此其输出的公式中可能包含一些在符号表示上与中国大陆教育体系现行规范不完全一致的情况，例如在特定物理符号表示、运算符号的样式或排列习惯等方面存在差异。为此，该后处理步骤通过一个LLM Agent，对大模型输出的 LaTeX 公式进行审查与调整，将其中可能出现的与中国教育场景不符的符号转换为我国GB 3102.11规范所定义的符号。通过这一机制，本系统能够生成更加符合学术规范的数学物理公式，并确保其在本土教育环境中的适用性。

== 图片生成

=== 图片生成原理

如果系统能够输出图片，有助于学生和老师对生成内容的理解，提高系统的泛用性。

尽管当前主流的LLM在文本处理方面表现优良，但在直接生成精确的、符合专业教学规范的图像方面仍存在显著局限。传统的图像生成模型，如基于扩散机制的模型（stable diffusion等），虽然能够生成高质量图像，但其计算资源消耗巨大、生成速度较慢，且本地化部署面临挑战。更为关键的是，此类模型通常更擅长生成艺术绘画类图像，对于需要高度准确性和逻辑性的专业教学图形，其生成效果往往较差。

@fig:混元模型生成电路图效果 是使用混元文生图模型进行的测试（提示词：“请为我生成一张用于基尔霍夫定律教学的电路图，该电路先将电阻 R2 与 R3 并联，再与 R1 和电源串联”）。从图中可以看出，生成的电路图在结构上与正确的电路图存在较大偏差，难以满足教学需求。

#figure(
  image("assets/混元模型生成电路图效果.jpeg", width: 50%),
  caption: [
    混元文生图模型生成电路图效果
  ],
) <混元模型生成电路图效果>

鉴于LLM在文本生成方面的优势，本系统提出一种间接的图形生成策略：利用模型生成结构化的文本描述，再将其渲染为图像。可缩放矢量图形（Scalable Vector Graphics, SVG）格式因其基于XML的标签化特性及易于浏览器渲染的优点，成为实现此策略的理想选择。SVG通过路径坐标等方式描述矢量图形，尤其适用于表达电路图这类逻辑结构清晰，元素单调的教学图像。

@fig:LLM生成svg示例 是使用Gemini 2.5 Pro Preview 05-06 生成的 svg 文本的渲染结果（提示词：“请使用 svg 为我生成一张用于基尔霍夫定律教学使用的电路图，该电路先将电阻 R2 与 R3 并联，再与 R1 和电源串联”）。该图不仅准确表示了电路结构，还包含了相关的教学公式，初步验证了该方法的可行性。

#figure(
  image("assets/svg生成示例.svg", width: 50%),
  caption: [
    LLM 生成 svg 示例
  ],
) <LLM生成svg示例>

进一步的实验表明，SVG代码生成的质量与LLM的逻辑推理及空间理解能力密切相关。在诸如Gemini 2.5 Pro、Deepseek R1及Claude 3.7 Sonnet等具备先进推理能力或 think 功能的较新模型上，SVG生成效果较为理想。相比之下，部分早期或轻量级模型（本系统中测试的本地部署Qwen 7B模型）在生成复杂SVG时表现欠佳。因此，在本系统的设计中，当选用本地部署的、能力相对受限的模型时，将默认禁用SVG图形生成功能，以保证输出内容的准确性与可靠性。

=== svg 生成过程

在系统测试阶段，LLM在生成SVG图形时，即使是利用了先进的在线模型，也常表现出图形与预期不符的问题。如@fig:错误svg示例 所示，生成的电路错误，未能精确反映用户意图，这影响了教学辅助材料的可用性。

#figure(
  image("assets/wrong_svg.svg", width: 50%),
  caption: [
    LLM 生成的错误 svg 示例
  ],
) <错误svg示例>

与上文的生成结果（@fig:LLM生成svg示例）对比分析，可以猜测该错误生成可能源于模型在处理长篇幅上下文时，注意力有限，难以同时兼顾文档内容的生成和SVG的生成，符合前文综述提到的“中间失焦”问题@liu2023lostmiddlelanguagemodels。当模型需要理解并生成复杂文本内容的同时，还要严格遵循SVG的语法规范来构建图形，这种多任务单次处理可能导致其在图形结构和细节上的表现力下降，引入逻辑偏差。

为解决这一问题，本系统采用了一种两阶段的SVG生成策略，旨在解耦内容生成与图形渲染过程，如@fig:svg生成过程 所示。在第一阶段，当系统识别出需要插入图形的场景时，LLM首先被引导生成该图形的自然语言描述。这一描述作为一种中间表示，仅包含图形的语义信息和关键视觉元素，避免了复杂的SVG语法细节。此步骤的优势在于，自然语言的生成对于LLM而言是其核心能力，且相较于代码生成，其对上下文长度的敏感度较低，从而确保了图形意图的准确传达。

#figure(
  image("assets/svg_processing.png", width: 100%),
  caption: [
    svg 生成过程
  ],
) <svg生成过程>

随后，在第二阶段，系统将提取出的自然语言描述独立提交给同一LLM的另一实例，专门用于将该描述转换为精确的SVG代码。这种方法使得LLM在生成SVG时能够专注于图形的语法和结构，而无需同时处理整个文档的上下文，从而降低任务的复杂性，提高了SVG生成的准确性。通过将复杂的生成任务分解为语义理解和格式转换两个相对独立的子任务，系统能够更有效地利用LLM的能力，减少因上下文过长而导致的生成错误。

为了提高系统整体效率，第二阶段的SVG生成过程可以并行执行。然而，鉴于大型语言模型API通常存在请求速率或并发连接数的限制（每分钟请求数或同时连接数上限），系统在设计时实施了信号量（Semaphore）并发控制机制。这确保了在不超出API限制的前提下最大化生成效率，避免因请求过载而导致的API错误或服务中断。

=== svg 渲染过程

系统的用户界面输出模块需同时支持Markdown文本、数学公式及SVG图形的渲染。本系统使用的前端Markdown渲染库react-markdown本身不直接支持SVG内嵌标签的解析，因此引入了rehype-raw插件。该插件的核心功能是将包含原始HTML（SVG标签）字符串的HTML抽象语法树（AST）节点解析为符合规范的、可被正确处理的语法树节点，从而实现SVG在前端界面中的无缝渲染。

在实践过程中还遇到了一个问题：部分由模型生成的、带有原生缩进的SVG代码在Markdown环境中渲染时，其内部的路径（\<path\>）等关键绘图元素会意外丢失，仅保留文本（\<text\>）部分的排列。通过对渲染后HTML源代码的排查，揭示了问题的成因：Markdown解析器在处理带有缩进的SVG代码时，可能错误地将SVG标签内部的缩进内容识别并转换为独立的HTML段落。这种转换破坏了SVG原有的层级结构和语法完整性，导致SVG内部的\<circle\>、\<text\>等子元素被错误地置于\<svg\>主标签之外。由于这些子元素并非HTML5标准所定义的合法标签，浏览器无法对其进行正确解析与渲染，最终造成图形显示不完整。

#figure(
  sourcecode[
    ```md
    1. 正文内容
       <svg width="300" height="200" xmlns="http://www.w3.org/2000/svg">
         <circle cx="100" cy="160" r="3" fill="black"/>
         <text x="105" y="175" font-size="12">C</text>
       </svg>
    2. 正文内容
    ```
  ],
  kind: image,
  caption: "遇到问题的 SVG 内容示例",
) <遇到问题的SVG内容示例>

例如，@fig:遇到问题的SVG内容示例 的 svg 可能被渲染为@fig:问题SVG渲染后的HTML代码 所示的 html 代码：

#figure(
  sourcecode[
    ```html
    <li>正文内容<br>
    <svg width="300" height="200" xmlns="http://www.w3.org/2000/svg"></svg><br>
    <circle cx="100" cy="160" r="3" fill="black"><br>
    <text x="105" y="175" font-size="12">C</text><br>
    </circle>
    </li>
    ```
  ],
  kind: image,
  caption: "问题 SVG 渲染后的 HTML 代码",
) <问题SVG渲染后的HTML代码>

可以看出理应在 svg 标签里的 circle 和 text 部件并没有被渲染到 svg 标签里，而 text 和 circle 都不是 html5 的合法标签，因此无法正确渲染。

为规避此渲染错误，本系统在SVG输出阶段增加了一道额外的正则表达式替换步骤。该步骤的核心逻辑如@fig:SVG渲染修复函数 所示的Python函数svg_to_single_line，其功能是将模型生成的多行SVG代码字符串中的换行符和不必要的空白字符移除，将其整理为单行字符串。通过此操作，可以有效防止Markdown解析器因缩进而错误地放置标签，从而确保SVG结构在前端界面中的正确渲染。

#figure(
  sourcecode[
    ```py
    def svg_to_single_line(markdown_string: str) -> str:
        """
        将 Markdown 字符串中所有的 <svg>...</svg> 元素转换为一行。
        """
        svg_pattern = re.compile(r"(<svg.*?>.*?</svg>)", re.S)

        def replace_svg_content(match):
            """
            re.sub 的回调函数，用于处理每个匹配到的 SVG 块。
            """
            # match.group(1) 是开标签 (<svg ... >)
            # match.group(2) 是 SVG 内容 (开标签和闭标签之间的内容)
            # match.group(3) 是闭标签 (</svg>)
            svg_region = match.group(1)
            cleaned_content = re.sub(r">[\s\n]+<", "><", svg_region)
            return cleaned_content

        transformed_string = svg_pattern.sub(replace_svg_content, markdown_string)
        return transformed_string
    ```],
  kind: image,
  caption: "SVG 渲染修复函数",
) <SVG渲染修复函数>

= 用户界面实现

用户界面作为用户与 Agentic RAG 系统交互的核心媒介，其设计的优劣直接影响系统的可用性与用户体验。该系统构建了一个直观、高效且功能完善的前端应用，以支持教师在教学内容生成过程中的各项需求。

为实现这一目标，前端技术选型综合考虑了开发效率、社区支持、性能以及代码可维护性。本系统采用了 React 框架作为核心，利用其声明式编程范式和基于组件的架构来构建可复用的 UI 单元。为了提升开发效率和界面美感，采用了 TailwindCSS 这一原子化 CSS 框架，它通过提供大量预设的工具类，使得开发者能够快速构建自定义设计而无需编写大量传统 CSS。同时，为了保证代码的健壮性和可维护性，项目全面采用 TypeScript 进行开发，通过静态类型检查减少潜在的运行时错误。在基础组件层面，本系统引入了 Radix UI 组件库，它提供了一系列无样式、高可访问性的 UI 原语，为构建定制化且符合 WCAG (Web Content Accessibility Guidelines) 标准的组件提供了基础。

前端应用的整体代码结构经过工程实践组织以确保模块化和可扩展性，如@fig:前端React组件结构 所示：

pages 目录是应用视图层的主体，存放了系统的三个核心功能页面：内容生成、知识库管理和模型配置。用户通过侧边栏（SideBar）中的导航链接在这些页面间进行切换，实现了清晰的单页面应用（SPA）路由体验。

lib 目录承担了与后端 Agentic RAG 服务进行通信的职责。该目录下的模块封装了所有对后端 API 的请求逻辑，例如获取知识文档、提交教案生成请求、接收流式生成结果等。

为配合 TypeScript 的静态类型检查，types 目录专门用于存放所有与 API 接口相关的数据结构类型定义，包括请求参数的结构、服务器响应的格式等。

config 目录则用于管理应用级别的配置信息，其中最核心的是后端服务的 API 地址。考虑到开发、测试和生产环境可能使用不同的后端实例，系统设计允许用户通过导航栏的下拉菜单动态选择或配置后端服务器地址。此选定的地址将作为全局配置，供 lib 目录中的 API 接口模块在发起网络请求时使用。

components 目录是通用和特定功能 UI 组件的集合库。为了进一步提升代码的组织性，该目录下根据功能或通用性划分了若干子目录：

- layout 子目录定义了应用整体的布局结构，包含了全局导航栏（NavBar）和侧边栏（SideBar）。NavBar 通常承载系统标题、用户状态以及前述的后端地址选择等功能；SideBar 则提供主要功能页面的导航入口。
- DocumentManager 子目录内封装了与文档管理相关的组件，这些组件构成了知识库管理页面的核心功能。例如，它包含了文档上传对话框、文档列表展示、文档状态指示以及删除确认等交互元素。
- MarkdownRenderer 是一个关键的展示组件，负责将后端 Agentic RAG 系统生成的 Markdown 格式教案内容，以及其中可能包含的 LaTeX 数学公式和 SVG 矢量图形，准确、美观地渲染到用户界面上。这对于教学内容中复杂符号和图表的呈现至关重要。
- ChatMessage 组件用于构建用户与系统进行生成式交互时的对话界面。当用户输入指令或提问，以及系统逐步生成教案内容时，该组件以消息气泡的形式展示用户信息。
- WorkflowTimeline 是一个特色组件，用于在内容生成过程中可视化展示 Agentic RAG 系统内部的工作流程，用于用户对生成过程的直观感知。

#figure(
  image("assets/架构_前端文件结构.png", width: 100%),
  caption: [
    前端 React 组件结构
  ],
) <前端React组件结构>

== 首页

首页采用前端经典的上部导航栏 + 左侧侧边栏 + 右侧内容栏的布局，如@fig:首页。

导航栏中包含了一个下拉框，可以选择该系统的不同内容生成模式，这些模式将在下一节介绍。

侧边栏中包含了系统的名称、版本与所有功能页面入口，点击条目即可跳转到对应页面，主要包括*内容生成页面*（Home）、*文档管理页面*（Documents）、*模型管理页面*（API Keys）。

#figure(
  image("assets/界面_首页.png", width: 100%),
  caption: [
    首页
  ],
) <首页>

== 内容生成页面

内容生成页面以一个自定义的 Card 组件为主体，提供了消息编辑与发送、清空当前对话上下文的操作，如@fig:内容生成页面生成中。

本系统在标题栏为用户提供了四种可选操作模式：*教案模式*、*Agent模式*、*自由模式*和*知识图谱模式*。作为核心功能，教案模式采用预设的Workflow流程对用户输入进行处理，实现结构化教案的自动生成。Agent模式作为简化版本，不经过Workflow流程，而是通过单一Agent进行工具调用和内容生成，主要适用于针对知识库内容的*定向查询*场景。自由模式则提供基础的对话功能，直接将用户输入传递给大语言模型处理，不涉及工具调用或教案生成等复杂功能。知识图谱模式则根据材料和用户关键词，基于 Agent 的结构化输出生成知识图谱。这种分层设计能够满足不同场景下的差异化需求，从结构化教案制作到简单问答都能提供相应支持。

在教案模式、Agent模式和知识图谱模式下，生成过程的展示使用WorkflowTimeline组件，模仿了 Grok deepsearch 的展示方式，将展示区域分为两部分，左边显示当前工作流进度，右边则是工作流中某个节点的输出展开。用户可以任意选择展开节点，查看该节点的详细输出。节点旁的状态指示物指示了当前节点的输出状态，包括等待中、正在生成、生成完成，或者在网络突然断开情况下会显示生成失败。


#figure(
  image("assets/界面_生成中.png", width: 100%),
  caption: [
    内容生成页面（生成中）
  ],
) <内容生成页面生成中>

生成完成后，WorkflowTimeline组件会自动折叠，让用户聚焦于最终的输出结果。用户也可以随时再展开节点展示区域，查看节点的输出，如@fig:内容生成页面最终结果。

#figure(
  image("assets/界面_生成完成.png", width: 100%),
  caption: [
    内容生成页面（最终结果）
  ],
) <内容生成页面最终结果>

@fig:agent模式 是 Agent 模式的生成结果，展示了系统的资料查询能力。

#figure(
  image("assets/界面_agent模式.png", width: 100%),
  caption: [
    Agent 模式生成结果（资料查询场景）
  ],
) <agent模式>


@fig:知识图谱模式生成结果 是知识图谱模式的生成结果。知识图谱建立于 Agent 之上，对于用户给出的关键词，Agent 通过多次向量数据库查询，将数据库中所有的相关文本提取后，交给 LLM 进行分析总结。LLM 的输出格式被限制在特定的 json 格式中，具体类型为 {"nodes":{"id":string}[],"edges":{"source":string,"target":string,"label"?:string}[]}，例如@fig:知识图谱返回结果样例 表示了一个有效的知识图谱结构。

#figure(
  image("assets/界面_知识图谱模式.png", width: 100%),
  caption: [
    知识图谱模式生成结果
  ],
) <知识图谱模式生成结果>

#figure(
  sourcecode[
    ```json
    {
      "nodes": [
        {"id": "无源元器件"},
        {"id": "电阻器"},
        {"id": "电容器"},
        {"id": "电感器"},
        {"id": "电路基本元件"}
      ],
      "edges": [
        {"source": "电阻器", "target": "无源元器件", "label": "是一种"},
        {"source": "电容器", "target": "无源元器件", "label": "是一种"},
        {"source": "电感器", "target": "无源元器件", "label": "是一种"},
        {"source": "无源元器件", "target": "电路基本元件", "label": "属于"}
      ]
    }
    ```],
  kind: image,
  caption: "一个合法的知识图谱返回结果示例",
) <知识图谱返回结果样例>

前端接收到后端数据后，使用 react-force-graph-2d 组件对节点和边进行渲染。该组件内置有节点物理引擎，可以任意缩放、拖拽知识图谱的节点，并且该组件经过本系统的个性化定制后，在鼠标悬浮时可以高亮显示与当前节点/边所连接的所有元素，方便定位查看，如@fig:知识图谱的渲染与可操作性示例。

#figure(
  image("assets/界面_知识图谱操作.png", width: 100%),
  caption: [
    知识图谱的渲染与可操作性示例
  ],
) <知识图谱的渲染与可操作性示例>

== 知识库管理页面

如@fig:文档管理页面 所示，知识库管理页面允许用户上传、删除、重命名、启用或禁用文档。上传文档后，文档自动进入处理阶段（processing），此时后端将接收到的文档进行OCR、切分、向量化并存到向量数据库中。该流程结束后，文档状态将变为已就位（ready），此时启用文档即可在内容生成页面使用其进行索引。

启用与禁用文档是知识库管理的核心功能之一，它允许用户灵活地控制哪些文档可以被用于内容生成，这可以人为辅助提高系统输出结果的相关性和准确性。

#figure(
  image("assets/界面_文档管理.png", width: 100%),
  caption: [
    文档管理页面
  ],
) <文档管理页面>

== 模型配置页面

模型配置页面允许用户配置当前系统的模型，包括在线 API 模型和本地模型，如@fig:模型配置页面。

在线 API 模型配置页面允许用户配置当前系统的在线 API 模型。用户需要先选择模型服务提供商；随后，将向后端发起请求，获取该服务商的所有支持的模型列表。前端接收到后端传回的模型列表后，让用户再次选择使用的模型，并填入 API Key，保存配置即可。

#figure(
  image("assets/界面_在线api.png", width: 100%),
  caption: [
    云服务 API 模型配置页面
  ],
) <模型配置页面>

本地模型配置页面会访问 ollama server 的模型列表，并让用户选择已下载到本地的模型，如@fig:本地模型配置页面。

同时，用户可以输入模型名称进行搜索，从 ollama 官网下载新模型。由于 ollama 官方并没有提供在线的模型列表 API，因此该部分是使用爬虫制作的。后端接收到用户的搜索请求后，会使用爬虫访问 https://ollama.com/library/{name}/tags 爬取所有符合模型的 tags，并在前端展示并提示下载。下载过程中，前端不断轮询下载进度，而后端则会不断从 ollama server 获取下载进度，下载完成后模型即加入可用列表。

#figure(
  image("assets/界面_本地模型.png", width: 100%),
  caption: [
    本地模型配置页面
  ],
) <本地模型配置页面>

= 结论与展望

== 结论

本文围绕“基于Agentic RAG的AI教学辅助设计系统”这一主题，成功设计并实现了一个原型系统。该系统旨在融合LLM的强大自然语言理解与生成能力与RAG技术从外部知识库获取准确信息的能力，并通过引入Agent机制赋予系统更高级的任务规划、工具调用和迭代优化能力。其核心目标是解决传统LLM在直接应用于教学内容生成时可能出现的“幻觉”、知识陈旧、缺乏领域深度以及难以处理复杂指令等问题，从而为教师提供一个智能、高效、可靠的教学设计伙伴。

在理论层面，本研究探讨了Agentic RAG在教育内容生成领域的可行性与优势，分析了其相较于传统 RAG在处理复杂、多步骤教学设计任务时的潜力。

在实践层面，系统实现了以下关键功能：

- 智能知识库构建与管理：支持多种格式文档上传，集成了针对教学文档优化的OCR组件，实现了文本的自动分块、向量化及高效检索。用户可以灵活管理和组合知识库，为RAG提供内容基础。
- Agentic RAG驱动的内容生成：研发了轻量级Agent封装库AgenticWrapper，支持结构化输出和工具调用（知识库搜索、维基百科查询）。通过线性Workflow编排多个Agent协同工作，实现对复杂教案生成任务的分解、规划与执行。
- 增强的多模态内容表达：系统不仅能生成文本教案，还支持LaTeX数学公式的生成与前端渲染，并通过引导LLM生成SVG代码间接实现了教学相关矢量图形的输出，丰富了教案的表现形式。
- 灵活的LLM集成与用户界面：设计了统一的LLM交互接口，支持无缝切换多种在线API模型和本地部署的开源模型。提供了用户友好的前端界面，用于文档管理、模型配置、内容生成交互及过程可视化。

#indent() 本系统的主要创新点包括：

- Agentic RAG在教学设计中的应用探索：将Agent的规划、工具使用和反思能力与RAG相结合，提升了AI在复杂教学内容生成任务上的表现。
- 轻量级Agent与工作流框架的自主研发：AgenticWrapper和Workflow类为快速构建和定制Agent应用提供了基础，兼顾了灵活性与可控性。
- 针对教学场景的实用功能设计：如优化的OCR、公式与SVG图形生成、多知识库管理等，力求贴近教师实际需求。
- 对本地与远程LLM的兼容支持：方便用户根据成本、性能、数据隐私等选择合适的LLM。
- 对于知识图谱的支持：设计了图谱数据结构，并提供了网页渲染及优秀的人机交互。

== 改进与展望

尽管本系统已初步实现预期功能并展现出良好潜力，但仍存在进一步的改进和扩展空间：

- 引入多模型协作机制：探索将不同特点的LLM（如快速响应的轻量级模型与能力强大的重量级模型）组合应用于工作流的不同阶段，以优化系统效率和成本。
- 多轮对话的持续优化：增强系统的多轮对话能力，允许教师通过连续的指令对生成的教案进行迭代修改和细化，使系统更像一个智能助手。
- 深化Agent能力与工具集：进一步丰富Agent可调用的工具，例如集成更专业的学科知识库、教学案例库，或引入评估工具对生成内容进行自动化的质量评估和反馈。
- 用户个性化与学习：允许用户自定义教案模板和偏好，并探索让系统学习特定教师的教学风格，生成更具个性化的教学辅助材料。

#indent() 随着LLM性能的不断提升，未来该系统将能够胜任更加复杂的教学任务。其功能将不再局限于单节课的教案生成，而是扩展到单元教学设计、跨学科主题学习活动规划等更宏观的教学层面。

进一步地，该系统还可整合面向学生的教学辅助功能。通过分析学生学习过程数据、实现自动化作业批改以及生成个性化学习反馈，系统将帮助教师更精准地把握学情，最终实现真正的因材施教。

综上所述，本系统不仅是对现有技术在教育领域应用的一次有益尝试和改进，更旨在为未来智能化教学辅助系统的发展提供新的思路和技术路径。通过持续的技术迭代与深入的教育应用探索，Agentic RAG 技术有望在提升教学效率和推动教学创新方面发挥越来越重要的作用。

#pagebreak()

#bibliography("./ref.bib")

#pagebreak()

= 致谢 <nonumber>

本论文的顺利完成，离不开众多师长、同学和朋友的鼎力支持与无私帮助，在此谨致以最诚挚的谢意。

首先，我要特别感谢我的导师xxx老师。从论文的选题、研究思路的构建，到实验设计、数据分析，乃至论文的最终撰写，xxx老师都给予了我悉心的指导和无微不至的关怀。

感谢我的同窗好友与舍友，在论文写作过程中，我们互相学习、共同探讨，他们的宝贵意见和建议对本论文的完善起到了积极的作用。大家的交流和讨论，不仅开阔了我的思路，也让我学到了很多不同的思考问题的方式。

此外，在论文的资料搜集、思路整理、代码编写过程中，大型语言模型如 Gemini 2.5 Pro 也为我提供了一些有益的参考和启发。

最后，我要感谢我的家人，他们一直是我最坚实的后盾。在我求学和从事研究工作的过程中，他们给予了我支持和鼓励，让我能够心无旁骛地投入到学习和研究之中。

本系统的代码已在 Github 开源：#link("https://github.com/lxl66566/Teaching-Assistant")。由于本人学识水平有限，论文与代码中难免存在疏漏和不足之处，恳请各位老师和同学批评指正。

#pagebreak()

= 附录 A 接口文档 <nonumber>

#set heading(numbering: "A.1")
#counter(heading).update(1)

== 概述

本文档描述了 Agentic RAG 教学辅助系统的 API 接口规范。

- *Base URL:* /api
- *数据格式:* JSON

== 接口列表

=== 知识库管理

==== 上传文档

- *URL*: /knowledge/upload
- *方法*: POST
- *描述*: 上传文档到知识库进行处理
- *请求参数*: FormData 格式
  ```typescript
  {
    file: File;                  // 文件（支持 PDF、DOCX、PPT、TXT 等格式）
    type: string;                // 文档类型
    description?: string;        // 描述（可选）
  }
  ```
- *响应*:
  ```json
  {
    "id": "string",
    "filename": "string",
    "type": "string",
    "size": "integer",
    "status": "processing",
    "created_at": "string"
  }
  ```

==== 获取文档处理状态

- *URL*: /knowledge/status/{document_id}
- *方法*: GET
- *描述*: 获取文档处理状态
- *请求参数*: 路径参数
  - document_id: 文档 ID
- *响应*:
  ```json
  {
    "id": "string",
    "status": "string", // processing, completed, failed
    "progress": "number", // 0-100
    "message": "string",
    "created_at": "string",
    "updated_at": "string"
  }
  ```

==== 获取知识库列表

- *URL*: /knowledge/list
- *方法*: GET
- *描述*: 获取知识库文档和分组列表
- *请求参数*: 查询参数
  ```typescript
  {
    type?: string;              // 文档类型（可选）
  }
  ```
- *响应*:
  ```json
  {
    "documents": [
      {
        "id": "string",
        "filename": "string",
        "enabled": "boolean",
        "type": "string",
        "description": "string",
        "status": "string",
        "created_at": "string"
      }
    ]
  }
  ```

==== 删除知识库文档

- *URL*: /knowledge/{document_id}
- *方法*: DELETE
- *描述*: 删除知识库中的文档
- *请求参数*: 路径参数
  - document_id: 文档 ID
- *响应*:
  ```json
  {
    "success": true,
    "message": "文档已成功删除"
  }
  ```

==== 更新文档信息（改名，改 enabled 状态）

- *URL*: /knowledge/{document_id}
- *方法*: PUT
- *描述*: 更新文档信息
- *请求参数*:
  ```typescript
  {
    new_name?: string;         // 新文档名字
    enabled?: boolean;         // 是否启用
  }
  ```
- *响应*:
  ```json
  {
    "id": "string",
    "filename": "string",
    "type": "string", // 文档类型（PDF、DOCX、PPT等）
    "description": "string", // 描述（可选）
    "status": "processing",
    "created_at": "string",
    "enabled": "boolean"
  }
  ```

=== 聊天接口

==== 发送聊天请求接口

- *URL*: /chat/send
- *方法*: POST
- *描述*: 发送聊天请求，启动聊天处理工作流，返回工作流 ID
- *请求参数*:
  ```typescript
  {
    content: string;            // 当前用户消息内容
    messages: Array<{           // 完整的消息历史记录
      role: "user" | "assistant";
      content: string;
    }>;
    mode?: "teaching plan" | "agent" | "free" | "graph"; // 当前模式，默认为 "teaching plan"
    options?: {
      temperature?: number;     // 温度参数（可选）
      max_tokens?: number;      // 最大生成token数（可选）
    }
  }
  ```
- *响应*:
  ```typescript
  {
    workflow_id: string; // 工作流ID，用于后续轮询结果
    message_id: string; // 消息ID
  }
  ```

==== 轮询聊天结果接口

- *URL*: /chat/poll/{workflow_id}
- *方法*: GET
- *描述*: 轮询指定工作流的处理结果，获取当前步骤和状态
- *路径参数*:
  - workflow_id: 工作流 ID
- *响应*:
  ```typescript
  {
    status: "processing" | "completed" | "error" | "calcelled";      // 工作流状态
    current_step: number;                              // 当前步骤索引（从0开始）
    total_steps: number;                               // 总步骤数
    steps: Array<{
      index: number;                                   // 步骤索引
      name: string;                                    // 步骤名称
      description: string;                             // 步骤描述
      status: "waiting" | "processing" | "completed" | "error"; // 步骤状态
      result?: string;                                 // 步骤结果（Markdown文本）
      error?: string;                                  // 错误信息（如果有）
    }>;
    final_content?: string;                            // 最终生成内容（仅当工作流完成时）
    error?: string;                                    // 错误信息（如果有）
  }
  ```

==== 取消工作流

- *URL*: /chat/cancel/{workflow_id}
- *方法*: POST
- *描述*: 取消指定工作流
- *路径参数*:
  - workflow_id: 工作流 ID
- *响应*:
  ```json
  {
    "success": true,
    "message": "工作流已取消"
  }
  ```

=== 模型管理

==== 获取模型服务商

- *URL*: /models/providers
- *方法*: GET
- *描述*: 获取系统支持的所有模型服务商
- *响应*:
  ```json
  {
    "provider": [
      "string" // 模型服务商
    ]
  }
  ```

==== 获取某个模型服务商的所有模型和其 api key

- *URL*: /models/providers/{provider}
- *方法*: GET
- *描述*: 获取某个模型服务商的所有模型和其 api
- *响应*:
  ```json
  {
    "model": [
      "string" // 模型名称
    ],
    "api_key": "string" // provider API 密钥
  }
  ```

==== 获取本地所有模型

- *URL*: /models/local
- *方法*: GET
- *描述*: 获取本地所有模型
- *响应*:
  ```json
  {
    "model": [
      {
        "name": "string", // 模型名称
        "status": "string", // 状态：ready/downloading/not_found
        "size": "string", // 模型大小（如果已下载）
        "digest": "string" // 模型标识符
      }
    ]
  }
  ```

==== 搜索可用模型

- *URL*: /models/search
- *方法*: GET
- *描述*: 搜索可下载的模型
- *请求参数*: 查询参数
  ```typescript
  {
    query: string; // 搜索关键词
  }
  ```
- *响应*:
  ```json
  {
    "models": [
      {
        "name": "string", // 模型名称（带 tag）
        "description": "string", // 模型描述 (optional)
        "size": "string", // 模型大小 (optional)
        "updated": "string" // 模型标识符 (optional)
      }
    ]
  }
  ```

==== 下载模型

- *URL*: /models/download
- *方法*: POST
- *描述*: 开始下载模型
- *请求参数*:
  ```json
  {
    "name": "string" // 模型名称
  }
  ```
- *响应*:
  ```json
  {
    "success": true,
    "message": "开始下载模型"
  }
  ```

==== 获取下载进度

- *URL*: /models/download/{model_name}/progress
- *方法*: GET
- *描述*: 获取模型下载进度
- *响应*:
  ```json
  {
    "status": "string", // downloading/completed/failed
    "progress": "number", // 0-100
    "message": "string" // 错误信息（如果有）
  }
  ```

==== 使用某个特定模型

- *URL*: /models/configure
- *方法*: POST
- *描述*: 配置远程模型 API 密钥或本地模型
- *请求参数*:
  ```typescript
  {
    type: "remote" | "local";
    name: string;                // 模型名称
    provider?: string;           // 模型服务商
    api_key?: string;            // 远程模型 API 密钥（可选）
  }
  ```
- *响应*:
  ```json
  {
    "success": true,
    "message": "模型配置已更新"
  }
  ```

==== 获取当前模型配置

- *URL*: /models/current
- *方法*: GET
- *描述*: 获取当前模型配置
- *响应*:
  ```json
  {
    "type": "string", // remote 或 local 或 null
    "name": "string", // 模型名称（如果 type 为 null，则为 null）
    "provider": "string" // 模型服务商（如果是本地模型，则为 null）
  }
  ```

=== 系统状态

==== 获取系统状态

- *URL*: /system/status
- *方法*: GET
- *描述*: 获取系统运行状态
- *响应*:
  ```json
  {
    "status": "running",
    "version": "string",
    "database_status": "connected",
    "knowledge_base_count": "integer",
    "document_count": "integer",
    "embedding_model": "string",
    "active_llm": "string"
  }
  ```

== 错误码定义

#three_line_table((
  ("错误码", "描述"),
  ("400", "请求参数错误"),
  ("404", "资源不存在"),
  ("500", "服务器内部错误"),
  ("503", "服务不可用，可能是模型服务未配置"),
))

