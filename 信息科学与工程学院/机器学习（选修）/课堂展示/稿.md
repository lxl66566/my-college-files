在机器学习中，向量是一种基本单位。它不仅仅存在于几何空间中，还广泛应用于数据表示、特征提取、模型训练等方方面面。从图像、文本到音频，一切数据都可以被表示成向量。几何体是向量，狗狗们是向量，一段文本也可以是向量。这次的展示也是围绕着限量进行的。

向量，在线性代数里大家都学过，但是在机器学习中向量有着一些不一样的特性。
首先向量的每一个维度都可以表示一个特征。例如左边的图，如果用第一位表示形状，用第二位表示颜色，那么我们就可以用一个二维向量来表示图上的不同几何体的情况。这里的几何体是理想的，所以只需要二维的向量就可以表示出所有情况。而有一些现实中的东西，比如狗狗，它们的特征非常多，有毛色，品种，体型等非常多的特征需要进行区分。因此，如果要完美的表示一个实际存在的物质，所需要的向量维度应该是无穷的。实际应用中，我们肯定不能用无穷的向量，我们只能用有穷的向量，这也可以算是一种采样。

然后文本也可以用向量表示，每一个单词就是一个向量。如果是中文的话，就可以为每一个汉字创建一个向量。Chatgpt 3 中的每一个单词的向量都有 12288 维，总共有 50257 个词。下面是一个例子，把一段话分割为单词，每个单词编码为一个向量。

机器学习中，由于向量指代的是一个自然存在的事物，其也是具有规律的。例如，向量的聚集性指的是距离越近的向量，它们的共同特征也就越明显。然后向量还有同向相似性，如果两个向量组的差值非常相似，那么这个差值向量所代表的变换也具有相似的特征，例如图上...。

那么如何从一个现实的具体事物中构造出向量呢？我们以文本转换为向量为例。这一过程又被称为 embedding。文本 Embedding 衡量文本字符串之间的相关性，是自然语言处理中的重要技术。通过将文本映射到高维向量空间，我们可以捕捉到词汇之间的语义关系。one hot 方法是最简单的词向量表示方法。每个词被表示为一个很长的向量，向量的长度等于词汇表的大小，其中只有一个元素是 1，其余都是 0。不同的词向量的 1 的位置是不一样的。也就是说，这些词向量两两正交，词与词之间不存在联系，无法表达词与词之间的相似性。这是 one hot 的一个问题。另一个缺点是向量的体积增长是单词数的平方，随着单词数的增加，向量表的体积会爆炸式增长。

于是有一些其他的词向量 embedding 方法。。（介绍 ppt）

但是 one hot 并不是一无是处。我们通过各种 embedding 方法最后都能得到一个词向量表，在查询某个词的向量时，将它 One hot 的结果与词向量表进行矩阵乘法，得到的正好是所需的词向量。

下面介绍一些向量在机器学习中非常常用的算法。这些算法真正应用了前面所述的向量特性。

首先是最简单的向量相似度。我们可以根据两个向量之间的距离来判断向量的相似度。这里展示的是两个最基本的距离，余弦距离计算向量的夹角，夹角越小，说明向量越相似。欧式距离是几何意义上向量的距离。除此之外还有曼哈顿距离、汉明距离等。这些距离在实际应用中都存在其意义。

解决了向量的相似度以后，我们就会想，给定一个向量。在海量的向量池中找到与这个向量最相似的向量。或者说距离最短的向量。这就是最近邻（KNN）问题。
（最近邻问题在实际应用中非常广泛。例如，我们想判断某个邮件是否是垃圾邮件。垃圾邮件的特征是它包含很多广告词汇，而正常邮件的词汇相对比较正常。因此我们只需要在邮件的向量池中找具有垃圾邮件特征的向量的近邻即可。有一个暴力解法，就是将每一个向量与其他所有向量进行距离计算，并从中挑选出最小距离向量。这个算法在向量增加时。消耗的时间也是成倍增加的。）

聚类算法是解决近邻问题的一个好手段。最经典的聚类算法就是 K means 算法。（K means 算法是机器学习中非常经典的算法。它将一个数据集划分为 K 个簇，使得簇内的数据点尽量相似。）这里是算法的演示。（根据 ppt 解释算法步骤）
这时候我们查找最近邻向量，只需要在给定向量的聚类内部比对查找就行了，不需要去全图搜索向量。

有了 K means 算法得到的聚类，我们甚至可以对向量进行压缩。只需要将每一个聚类区域的向量，用聚类中心点代替，就可以将所有的向量压缩到 K 个。这是一个例子，图片上像素点使用聚类压缩后，可以得到一张类似但是体积非常小的图像。图像的质量当然跟聚类的数目成正比。在压缩后的图像上很容易看出一些色块的同化。（当然这两张图像实际上是 jpg 自己的压缩算法，跟聚类不一样，此处仅供演示）

局部敏感哈希(Locality-Sensitive Hashing, LSH) LSH 所得到的是一个比 k-means 更加粗略的结果，主要运用到高维海量数据的快速近似查找。基本思想是：找到一个 hash 函数。将原始数据空间中的两个相邻数据点通过相同的映射或投影变换（projection）后，这两个数据点在新的数据空间中仍然相邻的概率很大，而不相邻的数据点被映射到同一个空间的概率很小。这样的哈希函数跟普通的哈希函数是有很大区别的。

这里是一个极为简单的 LSH 算法演示。（照着视频解释）如果需要分类高维向量，只需要使用 _维度 - 1_ 的切割方式即可，能够保证把全局空间切成两份。。
当然还有更多复杂的 LSH 算法，他们的碰撞概率更低，相似性判断远比这一个算法更准确。（例如 minhash）

## 分块思想

这里讲的两个最近邻的算法，其中都能看出一些分块的思想。比如聚类算法将全图查找缩小为聚类内的查找，LSH 算法将每个向量截成更小的部分，以获取更低的碰撞概率和检测准确率。包括我的世界加载、红石电路的检测也是分区块加载，而不是全地图加载。这种分块的思想有点像是在“降低算法的维度”。一个解决问题的算法复杂度过大，随着数据规模增长，耗时增长过于迅速，使用分块可以让算法降低复杂度的指数。...

## 以下不一定讲

<!-- Embedding 通常用于以下场景：
搜索（结果按查询字符串的相关性进行排序）
聚类（将文本字符串按相似性分组）
推荐（推荐具有相关文本字符串的项目）
异常检测（识别相关性较小的异常值）
多样性测量（分析相似度分布）
分类（文本字符串按其最相似的标签进行分类） -->
